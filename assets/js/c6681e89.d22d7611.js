"use strict";(self.webpackChunknpipeline=self.webpackChunknpipeline||[]).push([[4197],{18204:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>c,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"architecture/optimization-principles","title":"Optimization Principles - How NPipeline Achieves High Performance","description":"Deep dive into architectural and design decisions that enable NPipeline\'s exceptional performance characteristics.","source":"@site/docs/architecture/optimization-principles.md","sourceDirName":"architecture","slug":"/architecture/optimization-principles","permalink":"/docs/architecture/optimization-principles","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":13,"frontMatter":{"title":"Optimization Principles - How NPipeline Achieves High Performance","description":"Deep dive into architectural and design decisions that enable NPipeline\'s exceptional performance characteristics.","sidebar_position":13},"sidebar":"docsSidebar","previous":{"title":"Design Principles","permalink":"/docs/architecture/design-principles"},"next":{"title":"Execution Plan Caching","permalink":"/docs/architecture/execution-plan-caching"}}');var s=i(74848),t=i(28453);const c={title:"Optimization Principles - How NPipeline Achieves High Performance",description:"Deep dive into architectural and design decisions that enable NPipeline's exceptional performance characteristics.",sidebar_position:13},l=void 0,a={},o=[{value:"Optimization Principles: How NPipeline Achieves High Performance",id:"optimization-principles-how-npipeline-achieves-high-performance",level:2},{value:"The Performance Challenge",id:"the-performance-challenge",level:2},{value:"Core Optimization Principles",id:"core-optimization-principles",level:2},{value:"1. Plan-Based Execution Model",id:"1-plan-based-execution-model",level:3},{value:"2. Zero Reflection During Steady State",id:"2-zero-reflection-during-steady-state",level:3},{value:"3. ICountable Optimization for Memory Efficiency",id:"3-icountable-optimization-for-memory-efficiency",level:3},{value:"4. Streaming-First Design with Lazy Evaluation",id:"4-streaming-first-design-with-lazy-evaluation",level:3},{value:"5. Allocation Reduction and ValueTask Optimization",id:"5-allocation-reduction-and-valuetask-optimization",level:3},{value:"3. Cached Context Access (Per-Item Optimization)",id:"3-cached-context-access-per-item-optimization",level:3},{value:"6. Graph-Based Execution for Dependency Clarity",id:"6-graph-based-execution-for-dependency-clarity",level:3},{value:"7. Memory Layout and Cache Efficiency",id:"7-memory-layout-and-cache-efficiency",level:3},{value:"How These Principles Work Together",id:"how-these-principles-work-together",level:2},{value:"6. Execution Plan Caching",id:"6-execution-plan-caching",level:2},{value:"Combined Impact of All Optimizations",id:"combined-impact-of-all-optimizations",level:2},{value:"Measurable Results",id:"measurable-results",level:2},{value:"Trade-offs and When These Optimizations Matter",id:"trade-offs-and-when-these-optimizations-matter",level:2},{value:"When Optimization Matters Most",id:"when-optimization-matters-most",level:3},{value:"When Optimization Matters Less",id:"when-optimization-matters-less",level:3},{value:"See Also",id:"see-also",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"optimization-principles-how-npipeline-achieves-high-performance",children:"Optimization Principles: How NPipeline Achieves High Performance"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"This page explains WHY NPipeline is fast."})," For HOW TO optimize your specific pipelines, see ",(0,s.jsx)(n.a,{href:"/docs/advanced-topics/",children:"Advanced Topics"})," and ",(0,s.jsx)(n.a,{href:"/docs/advanced-topics/performance-hygiene",children:"Performance Hygiene"}),"."]}),"\n",(0,s.jsx)(n.p,{children:"Before understanding optimization principles, you should be familiar with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/core-concepts",children:"Core Concepts Overview"})," - Basic NPipeline concepts and terminology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/",children:"Architecture Overview"})," - Understanding NPipeline's internal architecture"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/core-concepts/pipeline-execution/execution-strategies",children:"Execution Strategies"})," - How nodes execute data"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["NPipeline's performance advantages don't come by accident. They're result of deliberate architectural decisions made at framework level. This document explains ",(0,s.jsx)(n.strong,{children:"why"})," behind NPipeline's design and how these choices combine to deliver measurable performance benefits."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"the-performance-challenge",children:"The Performance Challenge"}),"\n",(0,s.jsx)(n.p,{children:"Data processing frameworks face inherent tradeoffs:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flexibility"})," (supporting diverse use cases) vs. ",(0,s.jsx)(n.strong,{children:"Optimization"})," (pre-computing for specific scenarios)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Developer Experience"})," (intuitive APIs, reduced boilerplate) vs. ",(0,s.jsx)(n.strong,{children:"Performance"})," (minimal overhead, zero-cost abstractions)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"})," (preventing errors) vs. ",(0,s.jsx)(n.strong,{children:"Speed"})," (avoiding runtime checks)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Most frameworks compromise by making reasonable defaults but allowing flexibility. NPipeline takes a different approach: optimize for most common, highest-impact scenarios while maintaining flexibility for others."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"core-optimization-principles",children:"Core Optimization Principles"}),"\n",(0,s.jsx)(n.h3,{id:"1-plan-based-execution-model",children:"1. Plan-Based Execution Model"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Pre-compute everything that doesn't change per-item."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Traditional Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Interpreted: evaluate routing/execution strategy for every item\r\nforeach (var item in items)\r\n{\r\n    var strategy = DetermineStrategy(item);\r\n    var result = ExecuteStrategy(strategy, item);\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Compiled: determine execution plan once, execute same plan for all items\r\nvar executionPlan = CompileExecutionPlan(pipeline);\r\nforeach (var item in items)\r\n{\r\n    executionPlan.Execute(item);\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Eliminating per-item branching reduces CPU cache misses"}),"\n",(0,s.jsx)(n.li,{children:"Predictable instruction patterns improve branch prediction"}),"\n",(0,s.jsx)(n.li,{children:"The CPU pipeline can optimize hot path more effectively"}),"\n",(0,s.jsx)(n.li,{children:"In high-throughput scenarios: thousands of decisions per second become zero decisions"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact:"})," Measurable CPU efficiency improvement, especially on modern CPUs with deep pipelines."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"2-zero-reflection-during-steady-state",children:"2. Zero Reflection During Steady State"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Pay reflection cost upfront, then never again."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Reflection Overhead:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Runtime type introspection (method resolution, property access)"}),"\n",(0,s.jsx)(n.li,{children:"Dynamic method invocation via delegates"}),"\n",(0,s.jsx)(n.li,{children:"Argument marshalling and unmarshalling"}),"\n",(0,s.jsx)(n.li,{children:"GC pressure from temporary objects created during reflection"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Traditional Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Per-item reflection: look up methods, invoke dynamically\r\nforeach (var item in items)\r\n{\r\n    var method = GetMethod(item.GetType());  // \u2190 Reflection\r\n    method.Invoke(transform, new[] { item }); // \u2190 Dynamic dispatch\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Compile-time: pre-compiled delegates to actual methods\r\nvar compiledDelegate = CompileDelegate<T>(method);\r\nforeach (var item in items)\r\n{\r\n    compiledDelegate(item);  // \u2190 Direct, compiled dispatch\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reflection is expensive: 100-1000x slower than direct method calls"}),"\n",(0,s.jsx)(n.li,{children:"Pre-compiled delegates are statically typed, JIT-optimizable"}),"\n",(0,s.jsx)(n.li,{children:"Reflection GC pressure is eliminated during steady state"}),"\n",(0,s.jsx)(n.li,{children:"The JIT compiler can inline delegate calls"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact:"})," Particularly noticeable in scenarios with millions of items, where per-item reflection overhead becomes dominant cost."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"3-icountable-optimization-for-memory-efficiency",children:"3. ICountable Optimization for Memory Efficiency"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Know the size of your data upfront to make smart allocation decisions."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Problem:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"IEnumerable<T>"})," has no size information"]}),"\n",(0,s.jsx)(n.li,{children:"Buffers must be over-allocated or reallocated (expensive)"}),"\n",(0,s.jsx)(n.li,{children:"Collections often allocate larger capacity than needed (wasting memory)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline's Solution - ICountable:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"public interface ICountable\r\n{\r\n    long Count { get; }\r\n}\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Pipes and collections implementing ",(0,s.jsx)(n.code,{children:"ICountable"})," expose their size, enabling:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Allocate exactly the right buffer size, no overshooting\r\nif (input is ICountable countable)\r\n{\r\n    var buffer = new T[countable.Count];\r\n    // Fill buffer with no reallocation\r\n}\r\nelse\r\n{\r\n    // Fall back to dynamic resizing for unknown sizes\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Right-sized buffers = reduced memory waste"}),"\n",(0,s.jsx)(n.li,{children:"Fewer reallocations = reduced allocation pressure"}),"\n",(0,s.jsx)(n.li,{children:"Predictable memory usage = easier capacity planning"}),"\n",(0,s.jsx)(n.li,{children:"Smaller GC working set = better cache locality"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact:"})," Especially important for pipelines with large intermediate collections (batching, aggregation)."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"4-streaming-first-design-with-lazy-evaluation",children:"4. Streaming-First Design with Lazy Evaluation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Process data incrementally, never buffer unnecessarily."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Traditional Batch Processing Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Load entire dataset \u2192 Filter \u2192 Transform \u2192 Load output\r\n\u2193\r\nMemory usage = entire dataset in memory at once\r\n\u2193\r\nLarge GC pauses when data is collected\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline Streaming Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Item 1 \u2192 Filter \u2192 Transform \u2192 Output\r\nItem 2 \u2192 Filter \u2192 Transform \u2192 Output\r\nItem 3 \u2192 Filter \u2192 Transform \u2192 Output\r\n\u2193\r\nMemory usage = only current item + accumulated state\r\n\u2193\r\nTiny, predictable GC pauses\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implementation:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"IAsyncEnumerable<T>"})," for lazy evaluation"]}),"\n",(0,s.jsx)(n.li,{children:"Pull-based data flow (demand-driven)"}),"\n",(0,s.jsx)(n.li,{children:"State is only accumulated when explicitly required (aggregation, joins)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Memory usage scales with state complexity, not data volume"}),"\n",(0,s.jsx)(n.li,{children:"GC pause times are predictable and minimal"}),"\n",(0,s.jsx)(n.li,{children:"Latency for processing first item is low (no waiting for batch assembly)"}),"\n",(0,s.jsx)(n.li,{children:"Natural backpressure: slow consumers slow down producers"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact:"})," Enables processing of datasets far larger than available memory, with minimal latency impact."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"5-allocation-reduction-and-valuetask-optimization",children:"5. Allocation Reduction and ValueTask Optimization"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Eliminate unnecessary heap allocations in hot paths."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Problem - Traditional Async Framework:"})}),"\n",(0,s.jsxs)(n.p,{children:["In a pipeline processing 1M items/sec with 90% cache hits: ",(0,s.jsx)(n.strong,{children:"900,000 Task allocations per second"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline Approach - ValueTask:"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"ValueTask<T>"})," is a struct-based alternative to ",(0,s.jsx)(n.code,{children:"Task<T>"})," that:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Allocates on stack"})," (not heap) when result is available synchronously"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Zero allocations"})," for common case in cache-hit or synchronous scenarios"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Seamlessly transitions"})," to true async work when needed"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ValueTask<T>"})," is a struct (stack-allocated)"]}),"\n",(0,s.jsx)(n.li,{children:"For synchronous results: zero heap allocations"}),"\n",(0,s.jsxs)(n.li,{children:["For asynchronous results: seamlessly transitions to ",(0,s.jsx)(n.code,{children:"Task<T>"})]}),"\n",(0,s.jsx)(n.li,{children:"No performance penalty for async fallback path"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Measured Impact:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Up to 90% reduction in GC pressure"})," in high-cache-hit scenarios"]}),"\n",(0,s.jsx)(n.li,{children:"Smoother throughput: fewer GC pauses"}),"\n",(0,s.jsx)(n.li,{children:'More predictable latency: less "garbage spikes"'}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Common Scenarios:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Data validation (usually synchronous)"}),"\n",(0,s.jsx)(n.li,{children:"Filtering (usually synchronous)"}),"\n",(0,s.jsx)(n.li,{children:"Cached enrichment (high synchronous fast path rate)"}),"\n",(0,s.jsx)(n.li,{children:"These represent everyday pipeline tasks, not edge cases"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For complete implementation guidance, including critical constraints and real-world examples, see ",(0,s.jsx)(n.a,{href:"/docs/advanced-topics/synchronous-fast-paths",children:(0,s.jsx)(n.strong,{children:"Synchronous Fast Paths and ValueTask Optimization"})}),"\u2014the dedicated deep-dive guide that covers the complete implementation pattern and dangerous constraints you must understand."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"3-cached-context-access-per-item-optimization",children:"3. Cached Context Access (Per-Item Optimization)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Cache frequently-accessed execution context to eliminate per-item dictionary lookups."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Problem - Context Access Overhead:"})}),"\n",(0,s.jsx)(n.p,{children:"During high-throughput node execution (1M+ items/sec), accessing execution context properties multiple times per item adds up:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'// Before optimization: dictionary lookups for every item\r\nforeach (var item in items)\r\n{\r\n    var retryOptions = context.Items.TryGetValue("retry::nodeId", ...); // \u2190 Lookup 1\r\n    var tracingEnabled = context.Tracer is not NullTracer;              // \u2190 Lookup 2\r\n    var loggingEnabled = context.LoggerFactory is not NullFactory;      // \u2190 Lookup 3\r\n    // ... process item\r\n}\n'})}),"\n",(0,s.jsx)(n.p,{children:"With millions of items, these repeated lookups become a measurable bottleneck (150-250\u03bcs overhead per 1K items)."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline Solution - CachedNodeExecutionContext:"})}),"\n",(0,s.jsx)(n.p,{children:"Capture execution context state once at node scope, then reuse for all items:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// After optimization: capture once, reuse for all items\r\nvar cached = CachedNodeExecutionContext.Create(context, nodeId);\r\n\r\nforeach (var item in items)\r\n{\r\n    // Use cached values - no dictionary lookups\r\n    if (cached.TracingEnabled)\r\n    {\r\n        // ... trace\r\n    }\r\n    var maxRetries = cached.RetryOptions.MaxItemRetries; // \u2190 Already cached\r\n    // ... process item\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"How It Works:"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"CachedNodeExecutionContext"})," is a readonly struct that captures:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"NodeId"})," - Identifier of the executing node"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"RetryOptions"})," - Pre-resolved retry configuration (with precedence: node-specific \u2192 global \u2192 context)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"TracingEnabled"})," - Whether tracing is active (checked once)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LoggingEnabled"})," - Whether logging is active (checked once)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"CancellationToken"})," - Cancellation token for this execution"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Eliminates per-item dictionary lookups"})," - Direct field access instead of ",(0,s.jsx)(n.code,{children:"context.Items.TryGetValue()"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Zero allocation overhead"})," - Readonly struct is stack-allocated"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Transparent to users"})," - Automatically used by execution strategies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Thread-safe"})," - Each worker thread has its own cached instance"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Immutability Guarantee:"})}),"\n",(0,s.jsx)(n.p,{children:"The cached context assumes that execution state remains immutable during node execution. NPipeline enforces this automatically:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["In DEBUG builds: ",(0,s.jsx)(n.code,{children:"PipelineContextImmutabilityGuard"})," validates that context state hasn't changed"]}),"\n",(0,s.jsx)(n.li,{children:"In RELEASE builds: Zero overhead (validation compiled out)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If mutations are detected in DEBUG builds, a descriptive exception is thrown:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"Context immutability violation detected for node 'myNode': \r\nRetry options were modified during node execution.\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Performance Impact:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"~150-250\u03bcs reduction per 1K items in typical pipelines"}),"\n",(0,s.jsxs)(n.li,{children:["Improvement scales with:","\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Number of items (more items = more savings)"}),"\n",(0,s.jsx)(n.li,{children:"Item processing cost (overhead is more noticeable with fast transforms)"}),"\n",(0,s.jsx)(n.li,{children:"Retry configuration usage (more lookups = more savings)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"When This Helps Most:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"High-throughput scenarios (100K+ items/sec)"}),"\n",(0,s.jsx)(n.li,{children:"Frequent retry option lookups"}),"\n",(0,s.jsx)(n.li,{children:"Parallel execution strategies"}),"\n",(0,s.jsx)(n.li,{children:"Nodes with low per-item processing cost"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["For best practices on working with cached contexts and avoiding mutations, see ",(0,s.jsx)(n.a,{href:"/docs/advanced-topics/performance-hygiene#avoid-context-mutations-during-node-execution",children:"Performance Hygiene: Context Immutability"}),"."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"6-graph-based-execution-for-dependency-clarity",children:"6. Graph-Based Execution for Dependency Clarity"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Make execution flow explicit and optimizable."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why a Graph?"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Clarity:"})," Visual representation of data dependencies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimization:"})," Can identify parallelizable segments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Composability:"})," Nodes can be chained, reused, tested independently"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Debuggability:"})," Clear data provenance (lineage)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Execution Strategy:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'The graph is traversed once during the "plan compilation" phase'}),"\n",(0,s.jsx)(n.li,{children:"Execution strategy (sequential, parallel) is determined from graph structure"}),"\n",(0,s.jsx)(n.li,{children:"No per-item graph traversal overhead"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"7-memory-layout-and-cache-efficiency",children:"7. Memory Layout and Cache Efficiency"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Respect CPU cache behavior."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Decisions:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Value types for small data:"})," Structs with < 16 bytes avoid GC overhead"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Array-backed collections:"})," Better cache locality than linked lists"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Contiguous buffers:"})," CPU prefetcher can predict access patterns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Minimize indirection:"})," Reduce pointer chasing in hot paths"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// \u2713 GOOD: Value types, contiguous memory\r\npublic readonly struct Event\r\n{\r\n    public long Timestamp { get; }\r\n    public int Value { get; }\r\n}\r\n\r\n// LESS OPTIMAL: Reference types, scattered memory\r\npublic class Event\r\n{\r\n    public long Timestamp { get; set; }\r\n    public int Value { get; set; }\r\n}\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"how-these-principles-work-together",children:"How These Principles Work Together"}),"\n",(0,s.jsx)(n.p,{children:"The principles don't operate in isolation; they combine synergistically:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Plan-Based Execution\r\n  \u2193 Eliminates per-item decisions\r\n  \u251c\u2192 Enables JIT optimization\r\n  \u2514\u2192 Improves CPU cache behavior\r\n\r\nZero Reflection at Runtime\r\n  \u2193 Direct method dispatch\r\n  \u251c\u2192 Inlinable and optimizable by JIT\r\n  \u2514\u2192 Reduces memory allocations\r\n\r\nValueTask Optimization\r\n  \u2193 Eliminates allocations in fast paths\r\n  \u251c\u2192 Reduces GC pressure\r\n  \u2514\u2192 Smaller GC working set = better cache locality\r\n\r\nStreaming + Lazy Evaluation\r\n  \u2193 Process incrementally\r\n  \u251c\u2192 Predictable memory usage\r\n  \u2514\u2192 Minimal GC pauses\r\n\r\nICountable for Right-Sizing\r\n  \u2193 Allocate exactly what's needed\r\n  \u251c\u2192 Fewer reallocations\r\n  \u2514\u2192 Better memory cache behavior\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"6-execution-plan-caching",children:"6. Execution Plan Caching"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Pre-compile expression trees once, cache them indefinitely."]}),"\n",(0,s.jsx)(n.p,{children:"NPipeline compiles execution plans (expression trees) when a pipeline first runs. These plans are expensive to build (~300-500\u03bcs per pipeline) but can be cached and reused:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"What Gets Cached:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Pre-compiled delegates for transform execution"}),"\n",(0,s.jsx)(n.li,{children:"Expression trees for type conversions"}),"\n",(0,s.jsx)(n.li,{children:"Execution routing for joins and aggregates"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Result:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"First run:"})," Pay compilation cost (~1.9ms for small pipeline)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Subsequent runs:"})," Use cached plans (~0.4-0.5ms for small pipeline)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Improvement:"})," 75% reduction in execution time after warm-up"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"When Caching Applies:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Same pipeline definition runs multiple times \u2713 (typical case)"}),"\n",(0,s.jsx)(n.li,{children:"Pipeline definition changes between runs \u2717 (automatic cache miss)"}),"\n",(0,s.jsx)(n.li,{children:"Preconfigured nodes with state \u2717 (caching disabled for safety)"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact:"})," 250-450\u03bcs saved per run for pipelines with caching enabled."]}),"\n",(0,s.jsxs)(n.p,{children:["See ",(0,s.jsx)(n.a,{href:"/docs/architecture/execution-plan-caching",children:"Execution Plan Caching"})," for detailed architecture."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"combined-impact-of-all-optimizations",children:"Combined Impact of All Optimizations"}),"\n",(0,s.jsx)(n.p,{children:"These six principles work together to create exceptional performance:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-plaintext",children:"Plan-Based Execution\r\n  \u2193 Compile once, execute many times\r\n  \u251c\u2192 Zero per-item decision overhead\r\n  \u2514\u2192 Plan Caching amplifies this (75% reduction on warm-up)\r\n\r\nZero Reflection at Runtime\r\n  \u2193 Direct method dispatch\r\n  \u251c\u2192 Inlinable and optimizable by JIT\r\n  \u2514\u2192 Reduces memory allocations\r\n\r\nValueTask Optimization\r\n  \u2193 Eliminates allocations in fast paths\r\n  \u251c\u2192 Reduces GC pressure\r\n  \u2514\u2192 Smaller GC working set = better cache locality\r\n\r\nStreaming + Lazy Evaluation\r\n  \u2193 Process incrementally\r\n  \u251c\u2192 Predictable memory usage\r\n  \u2514\u2192 Minimal GC pauses\r\n\r\nICountable for Right-Sizing\r\n  \u2193 Allocate exactly what's needed\r\n  \u251c\u2192 Fewer reallocations\r\n  \u2514\u2192 Better memory cache behavior\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"measurable-results",children:"Measurable Results"}),"\n",(0,s.jsx)(n.p,{children:"The combination of these principles produces observable performance characteristics:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"Typical Benefit"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"GC Pause Duration"})}),(0,s.jsx)(n.td,{children:"50-80% reduction vs. naive async approach"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Memory Allocations"})}),(0,s.jsx)(n.td,{children:"Up to 90% fewer in cache-hit scenarios"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Throughput (items/sec)"})}),(0,s.jsx)(n.td,{children:"2-5x improvement vs. interpreted frameworks"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Latency (p99)"})}),(0,s.jsx)(n.td,{children:"More predictable, fewer spikes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"CPU Efficiency"})}),(0,s.jsx)(n.td,{children:"Better branch prediction, cache locality"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"trade-offs-and-when-these-optimizations-matter",children:"Trade-offs and When These Optimizations Matter"}),"\n",(0,s.jsx)(n.h3,{id:"when-optimization-matters-most",children:"When Optimization Matters Most"}),"\n",(0,s.jsx)(n.p,{children:"These optimizations provide most benefit in:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High-throughput scenarios:"})," Millions of items per second"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-tenant systems:"})," GC pauses directly impact other tenants"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time processing:"})," Latency spikes from GC pauses are unacceptable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Long-running processes:"})," Accumulated allocation pressure matters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency-sensitive workloads:"})," Predictable performance critical"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"when-optimization-matters-less",children:"When Optimization Matters Less"}),"\n",(0,s.jsx)(n.p,{children:"These optimizations have minimal impact if:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Throughput is low:"})," (< 1000 items/sec) - bottleneck is elsewhere"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Items are large:"})," (> 100KB) - allocation cost is tiny vs. processing cost"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Processing is CPU-bound:"})," GC pressure is secondary concern"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency spikes are acceptable:"})," SLAs allow for GC pauses"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/execution-plan-caching",children:"Execution Plan Caching"})," - How plan caching eliminates compilation overhead"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/advanced-topics/performance-hygiene",children:"Performance Hygiene"})," - Best practices for writing performant NPipeline code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/advanced-topics/synchronous-fast-paths",children:"Synchronous Fast Paths"})," - Master ValueTask patterns in your transform nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/component-architecture",children:"Component Architecture"})," - Understand how these principles are implemented in codebase"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/execution-flow",children:"Execution Flow"})," - How optimization principles affect data flow"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/performance-characteristics",children:"Performance Characteristics"})," - Measurable performance implications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/architectural-foundations",children:"Architecture: Architectural Foundations"})," - Fundamental architectural building blocks"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/execution-plan-caching",children:"Execution Plan Caching"}),":"]})," Deep dive into how compiled plans are cached"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"/docs/advanced-topics/performance-hygiene",children:"Performance Hygiene"}),":"]})," Best practices for writing performant NPipeline code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"/docs/advanced-topics/synchronous-fast-paths",children:"Synchronous Fast Paths"}),":"]})," Master ValueTask patterns in your transform nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/component-architecture",children:"Component Architecture"}),":"]})," Understand how these principles are implemented in codebase"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/performance-characteristics",children:"Performance Characteristics"}),":"]})," Understanding performance implications of different approaches"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>c,x:()=>l});var r=i(96540);const s={},t=r.createContext(s);function c(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);