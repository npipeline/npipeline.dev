"use strict";(self.webpackChunknpipeline=self.webpackChunknpipeline||[]).push([[2205],{28453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>o});var i=r(96540);const t={},a=i.createContext(t);function s(e){const n=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:n},e.children)}},56581:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"core-concepts/common-patterns","title":"Common Patterns","description":"Explore practical patterns and recipes for building effective NPipeline pipelines.","source":"@site/docs/core-concepts/common-patterns.md","sourceDirName":"core-concepts","slug":"/core-concepts/common-patterns","permalink":"/docs/core-concepts/common-patterns","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"Common Patterns","description":"Explore practical patterns and recipes for building effective NPipeline pipelines.","sidebar_position":8},"sidebar":"docsSidebar","previous":{"title":"Pipeline Context (PipelineContext)","permalink":"/docs/core-concepts/pipeline-context"},"next":{"title":"Streaming vs. Buffering","permalink":"/docs/core-concepts/streaming-vs-buffering"}}');var t=r(74848),a=r(28453);const s={title:"Common Patterns",description:"Explore practical patterns and recipes for building effective NPipeline pipelines.",sidebar_position:8},o="Common Patterns",c={},l=[{value:"Pattern 1: ETL Pipeline",id:"pattern-1-etl-pipeline",level:2},{value:"Scenario",id:"scenario",level:3},{value:"Pattern 2: Data Validation with Error Handling",id:"pattern-2-data-validation-with-error-handling",level:2},{value:"Scenario",id:"scenario-1",level:3},{value:"Pattern 3: Branch (fan-out) Processing",id:"pattern-3-branch-fan-out-processing",level:2},{value:"Scenario",id:"scenario-2",level:3},{value:"Pattern 4: Batch Processing",id:"pattern-4-batch-processing",level:2},{value:"Scenario",id:"scenario-3",level:3},{value:"Pattern 5: Conditional Routing",id:"pattern-5-conditional-routing",level:2},{value:"Scenario",id:"scenario-4",level:3},{value:"Pattern 6: Data Merging",id:"pattern-6-data-merging",level:2},{value:"Scenario",id:"scenario-5",level:3},{value:"Best Practices",id:"best-practices",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",br:"br",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"common-patterns",children:"Common Patterns"})}),"\n",(0,t.jsx)(n.p,{children:"This guide covers practical patterns and recipes for solving common data processing scenarios with NPipeline."}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Want to understand the principles first?"})," See ",(0,t.jsx)(n.a,{href:"/docs/core-concepts/best-practices",children:"Best Practices"})," to learn the reasoning and guidelines behind building good pipelines."]}),"\n"]}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Implementation-focused."}),' This guide answers the "how" - with working code examples and recipes.',(0,t.jsx)(n.br,{}),"\n",(0,t.jsx)(n.strong,{children:"Principle-focused"})," guides like ",(0,t.jsx)(n.a,{href:"/docs/core-concepts/best-practices",children:"Best Practices"}),' explain the "why" behind good pipeline design.']}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"pattern-1-etl-pipeline",children:"Pattern 1: ETL Pipeline"}),"\n",(0,t.jsx)(n.p,{children:"Extract, Transform, Load pipelines are a fundamental use case for NPipeline. This pattern demonstrates reading data from a source, transforming it, and writing to a destination."}),"\n",(0,t.jsx)(n.h3,{id:"scenario",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Extract customer data from a CSV file, enrich it with regional information, and load it into a database."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using NPipeline;\r\nusing NPipeline.DataFlow;\r\nusing NPipeline.DataFlow.DataPipes;\r\nusing NPipeline.Execution;\r\nusing NPipeline.Connectors.Csv;\r\nusing NPipeline.Nodes;\r\nusing NPipeline.Observability.Tracing;\r\nusing NPipeline.Pipeline;\r\n\r\n// Define your data models\r\npublic sealed record RawCustomer(int Id, string Name, string Email, string City);\r\npublic sealed record EnrichedCustomer(int Id, string Name, string Email, string City, string Region);\r\n\r\n// Define nodes\r\npublic sealed class CustomerCsvSource : CsvSourceNode<RawCustomer>\r\n{\r\n    public CustomerCsvSource() : base("customers.csv") { }\r\n}\r\n\r\npublic sealed class RegionEnricher : TransformNode<RawCustomer, EnrichedCustomer>\r\n{\r\n    private readonly Dictionary<string, string> _cityToRegion = new()\r\n    {\r\n        ["New York"] = "Northeast",\r\n        ["Los Angeles"] = "West",\r\n        ["Chicago"] = "Midwest",\r\n        ["Houston"] = "South"\r\n    };\r\n\r\n    public override Task<EnrichedCustomer> ExecuteAsync(\r\n        RawCustomer item,\r\n        PipelineContext context,\r\n        CancellationToken cancellationToken = default)\r\n    {\r\n        var region = _cityToRegion.TryGetValue(item.City, out var r) ? r : "Unknown";\r\n        var enriched = new EnrichedCustomer(item.Id, item.Name, item.Email, item.City, region);\r\n        return Task.FromResult(enriched);\r\n    }\r\n}\r\n\r\npublic sealed class DatabaseSink : SinkNode<EnrichedCustomer>\r\n{\r\n    public override async Task ExecuteAsync(\r\n        IDataPipe<EnrichedCustomer> input,\r\n        PipelineContext context,\r\n        IPipelineActivity parentActivity,\r\n        CancellationToken cancellationToken = default)\r\n    {\r\n        var customerCount = 0;\r\n        await foreach (var customer in input.WithCancellation(cancellationToken))\r\n        {\r\n            // In real application, insert into database\r\n            Console.WriteLine($"Saving: {customer.Name} ({customer.Region})");\r\n            customerCount++;\r\n        }\r\n        Console.WriteLine($"Loaded {customerCount} customers");\r\n    }\r\n}\r\n\r\n// Define the pipeline\r\npublic sealed class EtlPipeline : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        var source = builder.AddSource<CustomerCsvSource, RawCustomer>();\r\n        var transform = builder.AddTransform<RegionEnricher, RawCustomer, EnrichedCustomer>();\r\n        var sink = builder.AddSink<DatabaseSink, EnrichedCustomer>();\r\n\r\n        builder.Connect(source, transform);\r\n        builder.Connect(transform, sink);\r\n    }\r\n}\r\n\r\n// Execute the pipeline\r\npublic static class Program\r\n{\r\n    public static async Task Main()\r\n    {\r\n        var runner = PipelineRunner.Create();\r\n        await runner.RunAsync<EtlPipeline>();\r\n    }\r\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"pattern-2-data-validation-with-error-handling",children:"Pattern 2: Data Validation with Error Handling"}),"\n",(0,t.jsx)(n.p,{children:"This pattern demonstrates validating data and routing invalid items to a separate error stream."}),"\n",(0,t.jsx)(n.h3,{id:"scenario-1",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Validate product prices and separate invalid items for review."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'public sealed record Product(int Id, string Name, decimal Price);\r\npublic sealed record ValidationError(int ProductId, string Reason);\r\n\r\npublic sealed class PriceValidator : TransformNode<Product, Product>\r\n{\r\n    public override Task<Product> ExecuteAsync(\r\n        Product item,\r\n        PipelineContext context,\r\n        CancellationToken cancellationToken = default)\r\n    {\r\n        if (item.Price < 0)\r\n            throw new InvalidOperationException($"Product {item.Id} has negative price: {item.Price}");\r\n\r\n        if (item.Price > 100000)\r\n            throw new InvalidOperationException($"Product {item.Id} has suspicious price: {item.Price}");\r\n\r\n        return Task.FromResult(item);\r\n    }\r\n}\r\n\r\npublic sealed class ValidationPipeline : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        var source = builder.AddSource<ProductSource, Product>();\r\n        var validator = builder.AddTransform<PriceValidator, Product, Product>();\r\n        var validSink = builder.AddSink<ValidProductSink, Product>();\r\n\r\n        builder.Connect(source, validator);\r\n        builder.Connect(validator, validSink);\r\n    }\r\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Points:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use try-catch in transforms to catch validation errors"}),"\n",(0,t.jsx)(n.li,{children:"Implement error handlers at node level for fine-grained control"}),"\n",(0,t.jsx)(n.li,{children:"Route errors to separate error sinks using multiple outputs (if supported)"}),"\n",(0,t.jsx)(n.li,{children:"Log validation errors for audit and debugging"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"pattern-3-branch-fan-out-processing",children:"Pattern 3: Branch (fan-out) Processing"}),"\n",(0,t.jsx)(n.p,{children:"Process data through multiple independent transformations in parallel."}),"\n",(0,t.jsx)(n.h3,{id:"scenario-2",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Calculate different metrics (sum, average, count) on the same data stream."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'public sealed record SalesData(int Id, decimal Amount);\r\npublic sealed record Metrics(decimal Total, decimal Average, int Count);\r\n\r\npublic sealed class MetricsAggregator : SinkNode<SalesData>\r\n{\r\n    private decimal _total;\r\n    private int _count;\r\n\r\n    public override async Task ExecuteAsync(\r\n        IDataPipe<SalesData> input,\r\n        PipelineContext context,\r\n        IPipelineActivity parentActivity,\r\n        CancellationToken cancellationToken = default)\r\n    {\r\n        _total = 0;\r\n        _count = 0;\r\n\r\n        await foreach (var item in input.WithCancellation(cancellationToken))\r\n        {\r\n            _total += item.Amount;\r\n            _count++;\r\n        }\r\n\r\n        var average = _count > 0 ? _total / _count : 0;\r\n        var metrics = new Metrics(_total, average, _count);\r\n        Console.WriteLine($"Metrics: Total={metrics.Total}, Average={metrics.Average}, Count={metrics.Count}");\r\n    }\r\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Points:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Create separate sink nodes for different outputs"}),"\n",(0,t.jsx)(n.li,{children:"Use PipelineContext to pass shared state between nodes"}),"\n",(0,t.jsx)(n.li,{children:"Consider using aggregation nodes for complex calculations"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"pattern-4-batch-processing",children:"Pattern 4: Batch Processing"}),"\n",(0,t.jsx)(n.p,{children:"Process data in batches rather than individual items."}),"\n",(0,t.jsx)(n.h3,{id:"scenario-3",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Save records in batches of 100 to improve database performance."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'public sealed class BatchDatabaseSink : SinkNode<Customer>\r\n{\r\n    private const int BatchSize = 100;\r\n\r\n    public override async Task ExecuteAsync(\r\n        IDataPipe<Customer> input,\r\n        PipelineContext context,\r\n        IPipelineActivity parentActivity,\r\n        CancellationToken cancellationToken = default)\r\n    {\r\n        var batch = new List<Customer>(BatchSize);\r\n\r\n        await foreach (var customer in input.WithCancellation(cancellationToken))\r\n        {\r\n            batch.Add(customer);\r\n\r\n            if (batch.Count >= BatchSize)\r\n            {\r\n                await SaveBatchAsync(batch, cancellationToken);\r\n                batch.Clear();\r\n            }\r\n        }\r\n\r\n        // Save remaining items\r\n        if (batch.Count > 0)\r\n        {\r\n            await SaveBatchAsync(batch, cancellationToken);\r\n        }\r\n    }\r\n\r\n    private async Task SaveBatchAsync(List<Customer> batch, CancellationToken cancellationToken)\r\n    {\r\n        // In real application, save batch to database\r\n        Console.WriteLine($"Saving batch of {batch.Count} customers");\r\n        await Task.CompletedTask;\r\n    }\r\n}\n'})}),"\n",(0,t.jsx)(n.h2,{id:"pattern-5-conditional-routing",children:"Pattern 5: Conditional Routing"}),"\n",(0,t.jsx)(n.p,{children:"Route items to different sinks based on conditions."}),"\n",(0,t.jsx)(n.h3,{id:"scenario-4",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Send high-value orders for expedited processing and normal orders to standard processing."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'public sealed record Order(int Id, decimal Total);\r\n\r\npublic sealed class OrderRouter : TransformNode<Order, Order>\r\n{\r\n    public override Task<Order> ExecuteAsync(\r\n        Order item,\r\n        PipelineContext context,\r\n        CancellationToken cancellationToken = default)\r\n    {\r\n        // Mark the order with priority info in context\r\n        var priority = item.Total > 1000 ? "High" : "Normal";\r\n        Console.WriteLine($"Order {item.Id}: {priority} priority");\r\n        return Task.FromResult(item);\r\n    }\r\n}\n'})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Points:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Use context flags to mark items for different processing paths"}),"\n",(0,t.jsx)(n.li,{children:"Implement logic in sink nodes to route based on item properties"}),"\n",(0,t.jsx)(n.li,{children:"Consider multiple pipeline instances for complex routing scenarios"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"pattern-6-data-merging",children:"Pattern 6: Data Merging"}),"\n",(0,t.jsx)(n.p,{children:"Merge data from multiple sources into a single stream."}),"\n",(0,t.jsx)(n.h3,{id:"scenario-5",children:"Scenario"}),"\n",(0,t.jsx)(n.p,{children:"Combine customer data from multiple CSV files."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"public sealed class MultiSourcePipeline : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        // Create multiple source nodes\r\n        var source1 = builder.AddSource<CsvSource1, Customer>();\r\n        var source2 = builder.AddSource<CsvSource2, Customer>();\r\n        var source3 = builder.AddSource<CsvSource3, Customer>();\r\n\r\n        // Create merge point\r\n        var deduplicator = builder.AddTransform<CustomerDeduplicator, Customer, Customer>();\r\n        var sink = builder.AddSink<MergedSink, Customer>();\r\n\r\n        // Connect all sources to the same transform\r\n        builder.Connect(source1, deduplicator);\r\n        builder.Connect(source2, deduplicator);\r\n        builder.Connect(source3, deduplicator);\r\n        builder.Connect(deduplicator, sink);\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Key Points:"})}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Multiple sources can connect to the same transform"}),"\n",(0,t.jsx)(n.li,{children:"Implement deduplication logic in the transform if needed"}),"\n",(0,t.jsx)(n.li,{children:"Use PipelineContext to track which source items came from"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"best-practices",children:"Best Practices"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Keep transforms focused"})," - Each transform should do one thing well"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handle errors explicitly"})," - Use error handlers or separate error streams"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Monitor performance"})," - Profile your pipeline to identify bottlenecks"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Use dependency injection"})," - Inject services like loggers and databases"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Test each node"})," - Use InMemorySourceNode and InMemorySinkNode for testing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Consider memory usage"})," - Stream data; don't load everything into memory"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Document assumptions"})," - Make clear what input data shapes each node expects"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/extensions/dependency-injection",children:"Dependency Injection"})}),": Manage complex dependencies pipeline components"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/extensions/parallelism",children:"Parallelism"})}),": Speed up item processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/extensions/testing",children:"Testing Pipelines"})}),": Test pipeline components"]}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);