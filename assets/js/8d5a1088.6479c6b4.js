"use strict";(self.webpackChunknpipeline=self.webpackChunknpipeline||[]).push([[5658],{834:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"tooling/analyzers/data-processing","title":"Data Processing Analyzers","description":"Ensure proper input consumption and streaming patterns in pipeline nodes.","source":"@site/docs/tooling/analyzers/data-processing.md","sourceDirName":"tooling/analyzers","slug":"/tooling/analyzers/data-processing","permalink":"/docs/tooling/analyzers/data-processing","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Data Processing Analyzers","description":"Ensure proper input consumption and streaming patterns in pipeline nodes.","sidebar_position":4},"sidebar":"docsSidebar","previous":{"title":"Performance Analyzers","permalink":"/docs/tooling/analyzers/performance"},"next":{"title":"Best Practice Analyzers","permalink":"/docs/tooling/analyzers/best-practices"}}');var a=t(4848),r=t(8453);const s={title:"Data Processing Analyzers",description:"Ensure proper input consumption and streaming patterns in pipeline nodes.",sidebar_position:4},o=void 0,l={},c=[{value:"Data Processing Analyzers",id:"data-processing-analyzers",level:2},{value:"NP9205: Non-Streaming Patterns in SourceNode",id:"np9205-non-streaming-patterns-in-sourcenode",level:3},{value:"Performance Impact",id:"performance-impact",level:4},{value:"Problematic Patterns",id:"problematic-patterns",level:4},{value:"Solution: Use Streaming Patterns",id:"solution-use-streaming-patterns",level:4},{value:"Advanced Streaming Patterns",id:"advanced-streaming-patterns",level:4},{value:"Streaming with Transformation",id:"streaming-with-transformation",level:5},{value:"Streaming with Filtering",id:"streaming-with-filtering",level:5},{value:"When to Use Each Pattern",id:"when-to-use-each-pattern",level:4},{value:"Implementation Guide",id:"implementation-guide",level:4},{value:"NP9302: Input Parameter Not Consumed",id:"np9302-input-parameter-not-consumed",level:3},{value:"Why This Matters",id:"why-this-matters",level:4},{value:"Solution: Always Consume Input",id:"solution-always-consume-input",level:4},{value:"Common Input Consumption Patterns",id:"common-input-consumption-patterns",level:4},{value:"Best Practices for SinkNode",id:"best-practices-for-sinknode",level:4},{value:"Why Data Processing Analyzers Matter",id:"why-data-processing-analyzers-matter",level:2},{value:"Configuration",id:"configuration",level:2},{value:"See Also",id:"see-also",level:2}];function d(e){const n={a:"a",br:"br",code:"code",h2:"h2",h3:"h3",h4:"h4",h5:"h5",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h2,{id:"data-processing-analyzers",children:"Data Processing Analyzers"}),"\n",(0,a.jsx)(n.p,{children:"Data processing analyzers protect the integrity of data flow through your pipelines. They detect patterns that cause data loss, memory bloat, or improper stream handling."}),"\n",(0,a.jsx)(n.h3,{id:"np9205-non-streaming-patterns-in-sourcenode",children:"NP9205: Non-Streaming Patterns in SourceNode"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"ID:"})," ",(0,a.jsx)(n.code,{children:"NP9205"}),"\n",(0,a.jsx)(n.strong,{children:"Severity:"})," Warning",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"Category:"})," Performance"]}),"\n",(0,a.jsx)(n.p,{children:"This analyzer detects non-streaming patterns in SourceNode implementations that can lead to memory issues and poor performance. The analyzer identifies the following problematic patterns:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"List and Array allocation and population"})," in ExecuteAsync methods"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:".ToAsyncEnumerable()"})," calls on materialized collections"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Synchronous I/O operations"})," like File.ReadAllText, File.WriteAllBytes, etc."]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:".ToList() and .ToArray()"})," calls that materialize collections in memory"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"performance-impact",children:"Performance Impact"}),"\n",(0,a.jsx)(n.p,{children:"Non-streaming patterns in SourceNode implementations cause:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"High Memory Usage"}),": Loading entire datasets into memory can cause OutOfMemoryException with large files"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Poor Startup Performance"}),": Applications must wait for all data to be loaded before processing begins"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Increased GC Pressure"}),": Large collections create more garbage collection work"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Reduced Scalability"}),": Memory requirements grow linearly with data size"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Blocking I/O"}),": Synchronous operations block threads and reduce throughput"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"problematic-patterns",children:"Problematic Patterns"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'// \u274c PROBLEM: Materializing all data in memory\npublic class BadSourceNode : SourceNode<string>\n{\n    protected override async Task ExecuteAsync(IDataPipe<string> output, PipelineContext context, CancellationToken cancellationToken)\n    {\n        // NP9205: Allocating List<T> and populating it\n        var items = new List<string>();\n        \n        // Read all lines from file into memory\n        var lines = File.ReadAllLines("large-file.txt"); // NP9205: Synchronous I/O\n        \n        foreach (var line in lines)\n        {\n            items.Add(line);\n        }\n        \n        // NP9205: Materializing collection with ToList()\n        foreach (var item in items.ToList())\n        {\n            await output.ProduceAsync(item, cancellationToken);\n        }\n    }\n}\n\n// \u274c PROBLEM: Using ToAsyncEnumerable on materialized collection\npublic class AnotherBadSourceNode : SourceNode<int>\n{\n    protected override async Task ExecuteAsync(IDataPipe<int> output, PipelineContext context, CancellationToken cancellationToken)\n    {\n        // NP9205: Creating array and then converting to async enumerable\n        var numbers = Enumerable.Range(0, 1000000).ToArray(); // NP9205: Array allocation\n        \n        // NP9205: Using ToAsyncEnumerable on materialized collection\n        await foreach (var number in numbers.ToAsyncEnumerable())\n        {\n            await output.ProduceAsync(number, cancellationToken);\n        }\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h4,{id:"solution-use-streaming-patterns",children:"Solution: Use Streaming Patterns"}),"\n",(0,a.jsx)(n.p,{children:"For SourceNode implementations, use async IAsyncEnumerable with yield return for proper streaming:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'// \u2705 CORRECT: Using IAsyncEnumerable with yield return\npublic class GoodSourceNode : SourceNode<string>\n{\n    protected override async Task ExecuteAsync(IDataPipe<string> output, PipelineContext context, CancellationToken cancellationToken)\n    {\n        // Stream data line by line without materializing in memory\n        await foreach (var line in ReadLinesAsync("large-file.txt", cancellationToken))\n        {\n            await output.ProduceAsync(line, cancellationToken);\n        }\n    }\n    \n    // Helper method that yields lines one at a time\n    private async IAsyncEnumerable<string> ReadLinesAsync(string filePath, [EnumeratorCancellation] CancellationToken cancellationToken)\n    {\n        using var reader = new StreamReader(\n            new FileStream(filePath, FileMode.Open, FileAccess.Read, FileShare.Read, bufferSize: 4096, useAsync: true));\n        \n        string? line;\n        while ((line = await reader.ReadLineAsync(cancellationToken)) != null)\n        {\n            yield return line; // Stream one line at a time\n        }\n    }\n}\n\n// \u2705 CORRECT: Streaming from database\npublic class DatabaseSourceNode : SourceNode<DataRecord>\n{\n    private readonly IDbConnection _connection;\n    \n    public DatabaseSourceNode(IDbConnection connection)\n    {\n        _connection = connection;\n    }\n    \n    protected override async Task ExecuteAsync(IDataPipe<DataRecord> output, PipelineContext context, CancellationToken cancellationToken)\n    {\n        await using var command = _connection.CreateCommand();\n        command.CommandText = "SELECT Id, Name FROM DataRecords";\n        \n        await using var reader = await command.ExecuteReaderAsync(cancellationToken);\n        \n        while (await reader.ReadAsync(cancellationToken))\n        {\n            var record = new DataRecord\n            {\n                Id = reader.GetInt32(0),\n                Name = reader.GetString(1)\n            };\n            \n            await output.ProduceAsync(record, cancellationToken);\n        }\n    }\n}\n\n// \u2705 CORRECT: Generating data stream without materialization\npublic class NumberGeneratorSourceNode : SourceNode<int>\n{\n    private readonly int _start;\n    private readonly int _count;\n    \n    public NumberGeneratorSourceNode(int start, int count)\n    {\n        _start = start;\n        _count = count;\n    }\n    \n    protected override async Task ExecuteAsync(IDataPipe<int> output, PipelineContext context, CancellationToken cancellationToken)\n    {\n        for (int i = 0; i < _count; i++)\n        {\n            cancellationToken.ThrowIfCancellationRequested();\n            await output.ProduceAsync(_start + i, cancellationToken);\n            \n            // Optional: Add small delay to prevent overwhelming downstream nodes\n            if (i % 1000 == 0)\n            {\n                await Task.Delay(1, cancellationToken);\n            }\n        }\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h4,{id:"advanced-streaming-patterns",children:"Advanced Streaming Patterns"}),"\n",(0,a.jsx)(n.h5,{id:"streaming-with-transformation",children:"Streaming with Transformation"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:"// \u2705 GOOD: Streaming with transformation\npublic class TransformingSourceNode : SourceNode<ProcessedData>\n{\n    protected override async Task ExecuteAsync(IDataPipe<ProcessedData> output, PipelineContext context, CancellationToken cancellationToken)\n    {\n        await foreach (var rawItem in GetRawItemsAsync(cancellationToken))\n        {\n            // Transform item without materializing the entire collection\n            var processedItem = ProcessItem(rawItem);\n            await output.ProduceAsync(processedItem, cancellationToken);\n        }\n    }\n    \n    private async IAsyncEnumerable<RawData> GetRawItemsAsync([EnumeratorCancellation] CancellationToken cancellationToken)\n    {\n        // Stream raw items from source\n        await foreach (var item in ReadFromSourceAsync(cancellationToken))\n        {\n            yield return item;\n        }\n    }\n    \n    private ProcessedData ProcessItem(RawData raw)\n    {\n        // Synchronous transformation is fine for individual items\n        return new ProcessedData\n        {\n            Id = raw.Id,\n            Value = raw.Value * 2,\n            Timestamp = DateTime.UtcNow\n        };\n    }\n}\n"})}),"\n",(0,a.jsx)(n.h5,{id:"streaming-with-filtering",children:"Streaming with Filtering"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:"// \u2705 GOOD: Streaming with filtering\npublic class FilteringSourceNode : SourceNode<FilteredData>\n{\n    private readonly Func<DataItem, bool> _filter;\n    \n    public FilteringSourceNode(Func<DataItem, bool> filter)\n    {\n        _filter = filter;\n    }\n    \n    protected override async Task ExecuteAsync(IDataPipe<FilteredData> output, PipelineContext context, CancellationToken cancellationToken)\n    {\n        await foreach (var item in GetAllItemsAsync(cancellationToken))\n        {\n            // Filter items without materializing the entire collection\n            if (_filter(item))\n            {\n                var filteredItem = new FilteredData(item);\n                await output.ProduceAsync(filteredItem, cancellationToken);\n            }\n        }\n    }\n    \n    private async IAsyncEnumerable<DataItem> GetAllItemsAsync([EnumeratorCancellation] CancellationToken cancellationToken)\n    {\n        // Stream items from source\n        await foreach (var item in ReadFromSourceAsync(cancellationToken))\n        {\n            yield return item;\n        }\n    }\n}\n"})}),"\n",(0,a.jsx)(n.h4,{id:"when-to-use-each-pattern",children:"When to Use Each Pattern"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Scenario"}),(0,a.jsx)(n.th,{children:"Recommended Approach"}),(0,a.jsx)(n.th,{children:"Reason"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Large files/streams"}),(0,a.jsxs)(n.td,{children:[(0,a.jsx)(n.code,{children:"IAsyncEnumerable"})," with ",(0,a.jsx)(n.code,{children:"yield return"})]}),(0,a.jsx)(n.td,{children:"Minimal memory usage"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Database queries"}),(0,a.jsx)(n.td,{children:"Stream from database cursor"}),(0,a.jsx)(n.td,{children:"Avoid loading entire result set"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"API calls with pagination"}),(0,a.jsx)(n.td,{children:"Page through results"}),(0,a.jsx)(n.td,{children:"Process data as it arrives"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Data generation"}),(0,a.jsx)(n.td,{children:"Generate and yield items"}),(0,a.jsx)(n.td,{children:"No need to store all items"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Small datasets (< 1000 items)"}),(0,a.jsx)(n.td,{children:"Either approach is fine"}),(0,a.jsx)(n.td,{children:"Memory impact is negligible"})]})]})]}),"\n",(0,a.jsx)(n.h4,{id:"implementation-guide",children:"Implementation Guide"}),"\n",(0,a.jsx)(n.p,{children:"To implement streaming SourceNode implementations:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Identify non-streaming patterns"})," using the NP9205 analyzer"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Replace List and Array allocations"})," with ",(0,a.jsx)(n.code,{children:"IAsyncEnumerable"})," methods"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Convert synchronous I/O"})," to async equivalents (",(0,a.jsx)(n.code,{children:"File.ReadAllText"})," \u2192 ",(0,a.jsx)(n.code,{children:"File.ReadAllTextAsync"}),")"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Remove .ToAsyncEnumerable()"})," calls on materialized collections"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Use yield return"})," to stream items one at a time"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Add cancellation support"})," with ",(0,a.jsx)(n.code,{children:"[EnumeratorCancellation]"})," attribute"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"np9302-input-parameter-not-consumed",children:"NP9302: Input Parameter Not Consumed"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"ID:"})," ",(0,a.jsx)(n.code,{children:"NP9302"}),"\n",(0,a.jsx)(n.strong,{children:"Severity:"})," Error",(0,a.jsx)(n.br,{}),"\n",(0,a.jsx)(n.strong,{children:"Category:"})," Data Processing"]}),"\n",(0,a.jsx)(n.p,{children:"This analyzer detects when a SinkNode implementation overrides ExecuteAsync but doesn't consume the input parameter. Sink nodes are designed to process all items from the input data pipe, but your implementation ignores the input."}),"\n",(0,a.jsx)(n.p,{children:"This analyzer identifies these problematic patterns:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"SinkNode.ExecuteAsync override without input consumption"})," - The method doesn't use the input parameter"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Empty ExecuteAsync implementation"})," - The method returns without processing input"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"ExecuteAsync with only side effects"})," - The method performs operations but ignores input data"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"why-this-matters",children:"Why This Matters"}),"\n",(0,a.jsx)(n.p,{children:"SinkNode is the terminal component in a pipeline that processes all data flowing through it. When a SinkNode doesn't consume its input:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Loss"}),": Items in the input pipe are never processed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pipeline Inefficiency"}),": The pipeline moves data but the sink doesn't handle it"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resource Waste"}),": Memory and processing are used to move data that's never consumed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Unexpected Behavior"}),": Applications may appear to work but silently ignore data"]}),"\n"]}),"\n",(0,a.jsx)(n.h4,{id:"solution-always-consume-input",children:"Solution: Always Consume Input"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-csharp",children:'// \u2705 CORRECT: Process all items from input\npublic class MySinkNode : SinkNode<string>\n{\n    public override async Task ExecuteAsync(IDataPipe<string> input, PipelineContext context, CancellationToken cancellationToken)\n    {\n        // Process all items from input\n        await foreach (var item in input.WithCancellation(cancellationToken))\n        {\n            Console.WriteLine($"Processing: {item}");\n            // Save to database, write to file, etc.\n        }\n    }\n}\n\n// \u2705 CORRECT: Use DataPipe operations\npublic class CountingSinkNode : SinkNode<string>\n{\n    public override async Task ExecuteAsync(IDataPipe<string> input, PipelineContext context, CancellationToken cancellationToken)\n    {\n        // Count all items\n        var count = await input.CountAsync(cancellationToken);\n        Console.WriteLine($"Total items processed: {count}");\n    }\n}\n\n// \u2705 CORRECT: Handle empty input gracefully\npublic class RobustSinkNode : SinkNode<string>\n{\n    public override async Task ExecuteAsync(IDataPipe<string> input, PipelineContext context, CancellationToken cancellationToken)\n    {\n        var hasItems = false;\n        await foreach (var item in input.WithCancellation(cancellationToken))\n        {\n            hasItems = true;\n            Console.WriteLine($"Processing: {item}");\n        }\n        \n        if (!hasItems)\n        {\n            Console.WriteLine("No items to process");\n        }\n    }\n}\n\n// \u2705 CORRECT: Conditional processing with default input consumption\npublic class ConditionalSinkNode : SinkNode<string>\n{\n    private readonly bool _shouldProcess;\n    \n    public ConditionalSinkNode(bool shouldProcess)\n    {\n        _shouldProcess = shouldProcess;\n    }\n    \n    public override async Task ExecuteAsync(IDataPipe<string> input, PipelineContext context, CancellationToken cancellationToken)\n    {\n        if (_shouldProcess)\n        {\n            await foreach (var item in input.WithCancellation(cancellationToken))\n            {\n                Console.WriteLine($"Processing: {item}");\n            }\n        }\n        else\n        {\n            // Always consume input even when not processing\n            await foreach (var _ in input.WithCancellation(cancellationToken))\n            {\n                // Just consume the items\n            }\n        }\n    }\n}\n'})}),"\n",(0,a.jsx)(n.h4,{id:"common-input-consumption-patterns",children:"Common Input Consumption Patterns"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Pattern"}),(0,a.jsx)(n.th,{children:"Example"}),(0,a.jsx)(n.th,{children:"Use Case"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Process all items"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"await foreach (var item in input.WithCancellation(cancellationToken)) { ... }"})}),(0,a.jsx)(n.td,{children:"Standard processing of each item"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Count items"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"var count = await input.CountAsync(cancellationToken);"})}),(0,a.jsx)(n.td,{children:"When you only need the count"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Collect to list"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"var items = await input.ToListAsync(cancellationToken);"})}),(0,a.jsx)(n.td,{children:"When you need all items in memory"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"First item only"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"var first = await input.FirstAsync(cancellationToken);"})}),(0,a.jsx)(n.td,{children:"When you only need the first item"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Any items check"}),(0,a.jsx)(n.td,{children:(0,a.jsx)(n.code,{children:"var hasItems = await input.AnyAsync(cancellationToken);"})}),(0,a.jsx)(n.td,{children:"When you just need to check if input is non-empty"})]})]})]}),"\n",(0,a.jsx)(n.h4,{id:"best-practices-for-sinknode",children:"Best Practices for SinkNode"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Always consume the input"})," - Use ",(0,a.jsx)(n.code,{children:"await foreach"})," or other data pipe operations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pass cancellation token"})," - Use ",(0,a.jsx)(n.code,{children:"WithCancellation(cancellationToken)"})," for proper cancellation support"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Handle empty input"})," - Your code should work correctly even if the input pipe is empty"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Consider performance"})," - For large datasets, process items in a streaming fashion rather than collecting all items"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Don't silently ignore input"})," - Even if you don't need to process items, consume them to acknowledge receipt"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"why-data-processing-analyzers-matter",children:"Why Data Processing Analyzers Matter"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Memory Efficiency"}),": Streaming patterns use constant memory regardless of data size"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Better Performance"}),": Processing begins immediately without waiting for all data to load"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scalability"}),": Can handle arbitrarily large datasets without running out of memory"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Integrity"}),": All data flowing through pipelines is properly consumed"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Resource Utilization"}),": Lower GC pressure and better cache locality"]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,a.jsxs)(n.p,{children:["Adjust analyzer severity in ",(0,a.jsx)(n.code,{children:".editorconfig"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-ini",children:"# Treat non-streaming patterns as errors\ndotnet_diagnostic.NP9205.severity = error\n\n# Treat unconsummed input as errors\ndotnet_diagnostic.NP9302.severity = error\n"})}),"\n",(0,a.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/core-concepts/streaming-vs-buffering",children:"Streaming vs Buffering"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/core-concepts/data-pipes",children:"Data Pipes"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/docs/architecture/performance-characteristics",children:"Performance Characteristics"})}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var i=t(6540);const a={},r=i.createContext(a);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);