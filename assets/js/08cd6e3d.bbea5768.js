"use strict";(self.webpackChunknpipeline=self.webpackChunknpipeline||[]).push([[2508],{8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>a});var i=r(6540);const t={},s=i.createContext(t);function o(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(s.Provider,{value:n},e.children)}},8497:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"advanced-topics/performance-hygiene","title":"Performance Hygiene","description":"Best practices for building high-performance, low-allocation data pipelines with NPipeline.","source":"@site/docs/advanced-topics/performance-hygiene.md","sourceDirName":"advanced-topics","slug":"/advanced-topics/performance-hygiene","permalink":"/docs/advanced-topics/performance-hygiene","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Performance Hygiene","description":"Best practices for building high-performance, low-allocation data pipelines with NPipeline.","sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Advanced Topics","permalink":"/docs/advanced-topics/"},"next":{"title":"Synchronous Fast Paths and ValueTask Optimization","permalink":"/docs/advanced-topics/synchronous-fast-paths"}}');var t=r(4848),s=r(8453);const o={title:"Performance Hygiene",description:"Best practices for building high-performance, low-allocation data pipelines with NPipeline.",sidebar_position:2},a=void 0,c={},l=[{value:"1. Minimize Memory Allocations",id:"1-minimize-memory-allocations",level:2},{value:"Use <code>struct</code> for Small, Short-Lived Data",id:"use-struct-for-small-short-lived-data",level:3},{value:"Reuse Buffers",id:"reuse-buffers",level:3},{value:"Use <code>PipelineObjectPool</code> for Framework Collections",id:"use-pipelineobjectpool-for-framework-collections",level:3},{value:"2. Be Mindful of <code>async</code> and <code>await</code>",id:"2-be-mindful-of-async-and-await",level:2},{value:"Avoid <code>async</code> for Synchronous Work",id:"avoid-async-for-synchronous-work",level:3},{value:"Use <code>ValueTask&lt;T&gt;</code> for &quot;Fast Path&quot; Scenarios",id:"use-valuetaskt-for-fast-path-scenarios",level:3},{value:"3. Choose the Right Concurrency Strategy",id:"3-choose-the-right-concurrency-strategy",level:2},{value:"Advanced Parallel Configuration",id:"advanced-parallel-configuration",level:3},{value:"Queue Policy Options",id:"queue-policy-options",level:4},{value:"Output Buffer Control",id:"output-buffer-control",level:4},{value:"Ordering Considerations",id:"ordering-considerations",level:4},{value:"4. Streaming vs. Buffering",id:"4-streaming-vs-buffering",level:2},{value:"5. Use Benchmarking",id:"5-use-benchmarking",level:2},{value:"4. Avoid Context Mutations During Node Execution",id:"4-avoid-context-mutations-during-node-execution",level:2},{value:"The Context Immutability Guarantee",id:"the-context-immutability-guarantee",level:3},{value:"What You Must Not Do",id:"what-you-must-not-do",level:3},{value:"What You Should Do Instead",id:"what-you-should-do-instead",level:3},{value:"Why This Matters",id:"why-this-matters",level:3},{value:"DEBUG Validation",id:"debug-validation",level:3},{value:"Best Practices Summary",id:"best-practices-summary",level:3}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.p,{children:'NPipeline is designed for high performance, but building an efficient pipeline requires careful consideration of how you write your nodes and structure your data flow. "Performance hygiene" refers to the practice of writing code that is mindful of memory allocations, CPU usage, and data transfer overhead.'}),"\n",(0,t.jsxs)(n.blockquote,{children:["\n",(0,t.jsxs)(n.p,{children:["\u2139\ufe0f"," For specific optimization patterns and techniques, see ",(0,t.jsx)(n.a,{href:"/docs/advanced-topics/synchronous-fast-paths",children:"Synchronous Fast Paths"}),"."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"By following these best practices, you can ensure your pipelines run as fast and efficiently as possible."}),"\n",(0,t.jsx)(n.h2,{id:"1-minimize-memory-allocations",children:"1. Minimize Memory Allocations"}),"\n",(0,t.jsx)(n.p,{children:"In high-throughput data pipelines, memory allocation can become a major bottleneck. The .NET garbage collector (GC) is highly optimized, but frequent, large allocations can lead to GC pressure, causing pauses that hurt performance."}),"\n",(0,t.jsxs)(n.h3,{id:"use-struct-for-small-short-lived-data",children:["Use ",(0,t.jsx)(n.code,{children:"struct"})," for Small, Short-Lived Data"]}),"\n",(0,t.jsxs)(n.p,{children:["If you are passing small, simple data objects between nodes, consider using a ",(0,t.jsx)(n.code,{children:"struct"})," instead of a ",(0,t.jsx)(n.code,{children:"class"}),". Structs are value types and are allocated on the stack (in most cases), which avoids putting pressure on the GC."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Good for performance:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"// A struct is allocated on the stack, avoiding GC pressure.\r\npublic readonly struct Point\r\n{\r\n    public int X { get; }\r\n    public int Y { get; }\r\n\r\n    public Point(int x, int y)\r\n    {\r\n        X = x;\r\n        Y = y;\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Avoid for high-throughput data:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"// A class is allocated on the heap, creating work for the GC.\r\npublic class Point\r\n{\r\n    public int X { get; set; }\r\n    public int Y { get; set; }\r\n}\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Caveat:"})," Be mindful of the size of your structs. Large structs can lead to expensive copy operations. As a rule of thumb, structs are ideal for types that are small (e.g., under 16 bytes) and immutable."]}),"\n",(0,t.jsx)(n.h3,{id:"reuse-buffers",children:"Reuse Buffers"}),"\n",(0,t.jsx)(n.p,{children:"If your nodes process data in batches or chunks (e.g., reading from a network stream), reuse buffers instead of allocating a new one for each operation."}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"// Zero-allocation buffer reuse with direct processing\r\npublic async IAsyncEnumerable<ReadOnlyMemory<byte>> ProcessStream(Stream stream, CancellationToken cancellationToken)\r\n{\r\n    // Rent a buffer from the ArrayPool to avoid allocations\r\n    var buffer = ArrayPool<byte>.Shared.Rent(8192);\r\n    try\r\n    {\r\n        int bytesRead;\r\n        while ((bytesRead = await stream.ReadAsync(buffer, 0, buffer.Length, cancellationToken)) > 0)\r\n        {\r\n            // Return a ReadOnlyMemory slice - no allocation!\r\n            yield return new ReadOnlyMemory<byte>(buffer, 0, bytesRead);\r\n        }\r\n    }\r\n    finally\r\n    {\r\n        // Return the buffer to the pool for reuse\r\n        ArrayPool<byte>.Shared.Return(buffer);\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:"For even better performance, process the data directly without yielding:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"// Process data inline for maximum performance\r\npublic async Task ProcessStreamInline(Stream stream, Func<ReadOnlyMemory<byte>, Task> processor, CancellationToken cancellationToken)\r\n{\r\n    var buffer = ArrayPool<byte>.Shared.Rent(8192);\r\n    try\r\n    {\r\n        int bytesRead;\r\n        while ((bytesRead = await stream.ReadAsync(buffer, 0, buffer.Length, cancellationToken)) > 0)\r\n        {\r\n            // Process immediately without yielding\r\n            await processor(new ReadOnlyMemory<byte>(buffer, 0, bytesRead));\r\n        }\r\n    }\r\n    finally\r\n    {\r\n        ArrayPool<byte>.Shared.Return(buffer);\r\n    }\r\n}\n"})}),"\n",(0,t.jsxs)(n.h3,{id:"use-pipelineobjectpool-for-framework-collections",children:["Use ",(0,t.jsx)(n.code,{children:"PipelineObjectPool"})," for Framework Collections"]}),"\n",(0,t.jsxs)(n.p,{children:["NPipeline now pools all of the hot-path dictionaries that back graph execution and ",(0,t.jsx)(n.code,{children:"PipelineContext"}),". You can rely on ",(0,t.jsx)(n.code,{children:"PipelineObjectPool"})," to rent these collections instead of allocating new instances for every run:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"var nodeOutputs = PipelineObjectPool.RentNodeOutputDictionary(graph.Nodes.Count);\r\n\r\ntry\r\n{\r\n    // Execute nodes using the shared dictionary...\r\n}\r\nfinally\r\n{\r\n    foreach (var (_, pipe) in nodeOutputs)\r\n    {\r\n        await pipe?.DisposeAsync();\r\n    }\r\n\r\n    nodeOutputs.Clear();\r\n    PipelineObjectPool.Return(nodeOutputs);\r\n}\n"})}),"\n",(0,t.jsxs)(n.p,{children:["When you build contexts manually, allow ",(0,t.jsx)(n.code,{children:"PipelineContext"})," to rent pooled dictionaries by not supplying ",(0,t.jsx)(n.code,{children:"Parameters"}),", ",(0,t.jsx)(n.code,{children:"Items"}),", or ",(0,t.jsx)(n.code,{children:"Properties"})," explicitly. Always dispose the context you create so the dictionaries are returned to the pool:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"await using var context = new PipelineContext(PipelineContextConfiguration.WithCancellation(cancellationToken));\r\n\r\nawait runner.RunAsync<MyPipeline>(context, cancellationToken);\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Supplying custom dictionaries is still supported\u2014",(0,t.jsx)(n.code,{children:"PipelineContext"})," detects ownership and will skip pool returns for caller-managed instances. This lets you decide which allocations are worth pooling while still benefiting from the framework defaults."]}),"\n",(0,t.jsxs)(n.h2,{id:"2-be-mindful-of-async-and-await",children:["2. Be Mindful of ",(0,t.jsx)(n.code,{children:"async"})," and ",(0,t.jsx)(n.code,{children:"await"})]}),"\n",(0,t.jsxs)(n.p,{children:["While ",(0,t.jsx)(n.code,{children:"async/await"})," is essential for I/O-bound work, it does introduce some overhead."]}),"\n",(0,t.jsxs)(n.h3,{id:"avoid-async-for-synchronous-work",children:["Avoid ",(0,t.jsx)(n.code,{children:"async"})," for Synchronous Work"]}),"\n",(0,t.jsxs)(n.p,{children:["If a method doesn't perform any truly asynchronous operations, don't mark it as ",(0,t.jsx)(n.code,{children:"async"}),". You can return a completed ",(0,t.jsx)(n.code,{children:"Task"})," directly."]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Good:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"public Task<int> GetConstantAsync()\r\n{\r\n    // No await needed, so no async state machine is generated.\r\n    return Task.FromResult(42);\r\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Avoid:"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"public async Task<int> GetConstantAsync()\r\n{\r\n    // Unnecessary async/await creates overhead.\r\n    return await Task.FromResult(42);\r\n}\n"})}),"\n",(0,t.jsxs)(n.h3,{id:"use-valuetaskt-for-fast-path-scenarios",children:["Use ",(0,t.jsx)(n.code,{children:"ValueTask<T>"}),' for "Fast Path" Scenarios']}),"\n",(0,t.jsxs)(n.p,{children:["If your method is often able to return a result synchronously (e.g., from a cache), but may sometimes need to be asynchronous, use ",(0,t.jsx)(n.code,{children:"ValueTask<T>"}),". This avoids a heap allocation for the ",(0,t.jsx)(n.code,{children:"Task"})," object in the synchronous case."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"This is especially critical for transformer nodes in high-volume pipelines."})," Many transforms are synchronous or have a high synchronous fast path (cache hits, simple mappings). Using ",(0,t.jsx)(n.code,{children:"Task<T>"})," for these transforms creates millions of unnecessary heap allocations per second, causing constant GC pressure."]}),"\n",(0,t.jsxs)(n.p,{children:["For comprehensive implementation guidance, including critical constraints and real-world examples, see ",(0,t.jsx)(n.a,{href:"/docs/advanced-topics/synchronous-fast-paths",children:(0,t.jsx)(n.strong,{children:"Synchronous Fast Paths and ValueTask Optimization"})}),"\u2014the dedicated deep-dive guide that covers the complete implementation pattern, performance impact quantification, and dangerous constraints you must understand."]}),"\n",(0,t.jsx)(n.h2,{id:"3-choose-the-right-concurrency-strategy",children:"3. Choose the Right Concurrency Strategy"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"I/O-Bound Work:"})," For nodes that spend most of their time waiting for network or disk I/O, use the ",(0,t.jsx)(n.a,{href:"/docs/extensions/parallelism",children:"Parallelism Extension"})," with a relatively high ",(0,t.jsx)(n.code,{children:"MaxDegreeOfParallelism"}),". This ensures that while some tasks are waiting, others are actively being processed."]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"CPU-Bound Work:"})," For nodes performing intensive calculations, set ",(0,t.jsx)(n.code,{children:"MaxDegreeOfParallelism"})," to a value close to ",(0,t.jsx)(n.code,{children:"Environment.ProcessorCount"}),". Note that this is already the default behavior when no value is specified, so you typically don't need to set it explicitly."]}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"advanced-parallel-configuration",children:"Advanced Parallel Configuration"}),"\n",(0,t.jsxs)(n.p,{children:["The Parallelism Extension provides fine-grained control over execution behavior through the ",(0,t.jsx)(n.code,{children:"ParallelOptions"})," class:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"using NPipeline.Extensions.Parallelism;\r\n\r\n// Configure parallel execution with advanced options\r\nvar parallelOptions = new ParallelOptions\r\n{\r\n    MaxDegreeOfParallelism = Environment.ProcessorCount, // Default when null\r\n    MaxQueueLength = 1000, // Controls backpressure by limiting input queue size\r\n    QueuePolicy = BoundedQueuePolicy.Block, // Behavior when queue is full\r\n    OutputBufferCapacity = 500, // Throttles workers by limiting output buffer\r\n    PreserveOrdering = true // Default: maintains input ordering in output\r\n};\r\n\r\n// Apply to a specific transform node\r\nbuilder.WithParallelOptions(transformHandle, parallelOptions);\n"})}),"\n",(0,t.jsx)(n.h4,{id:"queue-policy-options",children:"Queue Policy Options"}),"\n",(0,t.jsxs)(n.p,{children:["When ",(0,t.jsx)(n.code,{children:"MaxQueueLength"})," is set, you can control behavior when the queue becomes full:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"Block"})})," (default): The producer blocks until space is available, providing natural backpressure"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"DropNewest"})}),": Incoming items are discarded when the queue is full"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.code,{children:"DropOldest"})}),": The oldest items in the queue are discarded to make room for new ones"]}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"output-buffer-control",children:"Output Buffer Control"}),"\n",(0,t.jsxs)(n.p,{children:["The ",(0,t.jsx)(n.code,{children:"OutputBufferCapacity"})," option creates an additional throttling mechanism:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"When specified, it limits how many processed results can queue ahead of downstream consumption"}),"\n",(0,t.jsx)(n.li,{children:"This restores end-to-end backpressure when downstream nodes are slow"}),"\n",(0,t.jsx)(n.li,{children:"When null (default), output buffering is unbounded, which can lead to memory accumulation under sustained load"}),"\n"]}),"\n",(0,t.jsx)(n.h4,{id:"ordering-considerations",children:"Ordering Considerations"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["The ",(0,t.jsx)(n.code,{children:"PreserveOrdering"})," flag (default: true) ensures output maintains input order"]}),"\n",(0,t.jsx)(n.li,{children:"Setting it to false can increase throughput but results in unordered output"}),"\n",(0,t.jsxs)(n.li,{children:["Note: Drop-policy paths (",(0,t.jsx)(n.code,{children:"DropNewest"}),", ",(0,t.jsx)(n.code,{children:"DropOldest"}),") are inherently unordered regardless of this setting"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"4-streaming-vs-buffering",children:"4. Streaming vs. Buffering"}),"\n",(0,t.jsx)(n.p,{children:"NPipeline is designed around a streaming-first philosophy. Nodes should process items as they arrive and yield results immediately. Avoid collecting all items from the input into a list before processing."}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Good (Streaming):"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"public async IAsyncEnumerable<string> ExecuteAsync(IAsyncEnumerable<string> input, CancellationToken cancellationToken)\r\n{\r\n    // Process items as they come in.\r\n    await foreach (var item in input.WithCancellation(cancellationToken))\r\n    {\r\n        yield return item.ToUpper();\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.strong,{children:"Avoid (Buffering):"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"public async IAsyncEnumerable<string> ExecuteAsync(IAsyncEnumerable<string> input, CancellationToken cancellationToken)\r\n{\r\n    // This buffers the entire input into memory before processing.\r\n    // It can lead to high memory usage and delays the start of processing.\r\n    var allItems = await input.ToListAsync(cancellationToken);\r\n\r\n    foreach (var item in allItems)\r\n    {\r\n        yield return item.ToUpper();\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(n.p,{children:"Buffering is only appropriate if your logic requires access to the entire dataset at once (e.g., sorting or calculating a global aggregate)."}),"\n",(0,t.jsx)(n.h2,{id:"5-use-benchmarking",children:"5. Use Benchmarking"}),"\n",(0,t.jsxs)(n.p,{children:["The most reliable way to improve performance is to measure it. Use tools like ",(0,t.jsx)(n.a,{href:"https://benchmarkdotnet.org/",children:"BenchmarkDotNet"})," to write micro-benchmarks for your critical nodes. This allows you to test different implementations and configurations to see which performs best."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"[MemoryDiagnoser] // Track memory allocations\r\npublic class MyTransformBenchmarks\r\n{\r\n    private MyTransform _transform;\r\n    private IAsyncEnumerable<string> _data;\r\n\r\n    [Params(100, 1000)]\r\n    public int N;\r\n\r\n    [GlobalSetup]\r\n    public void Setup()\r\n    {\r\n        _transform = new MyTransform();\r\n        _data = new EnumerableSourceNode<string>(Enumerable.Range(0, N).Select(i => i.ToString())).GetAsyncEnumerator();\r\n    }\r\n\r\n    [Benchmark]\r\n    public async Task Transform()\r\n    {\r\n        await foreach(var _ in _transform.ExecuteAsync(_data))\r\n        {\r\n            // Consume results\r\n        }\r\n    }\r\n}\n"})}),"\n",(0,t.jsx)(n.h2,{id:"4-avoid-context-mutations-during-node-execution",children:"4. Avoid Context Mutations During Node Execution"}),"\n",(0,t.jsx)(n.h3,{id:"the-context-immutability-guarantee",children:"The Context Immutability Guarantee"}),"\n",(0,t.jsxs)(n.p,{children:["NPipeline uses ",(0,t.jsx)(n.code,{children:"CachedNodeExecutionContext"})," to optimize per-item context access by caching execution state (retry options, tracing configuration, etc.) at the start of node execution. This provides significant performance benefits, but it requires that context state remains ",(0,t.jsx)(n.strong,{children:"immutable during node execution"}),"."]}),"\n",(0,t.jsx)(n.h3,{id:"what-you-must-not-do",children:"What You Must Not Do"}),"\n",(0,t.jsxs)(n.p,{children:["\u274c ",(0,t.jsx)(n.strong,{children:"Don't modify retry options during node execution:"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"public async Task<TOut> ExecuteAsync(TIn item, PipelineContext context, CancellationToken cancellationToken)\r\n{\r\n    // \u274c WRONG: Context state is cached at node start; this modification may be ignored\r\n    context.Items[PipelineContextKeys.NodeRetryOptions(context.CurrentNodeId)] = newRetryOptions;\r\n    \r\n    // Proceed with transform\r\n    var result = TransformItem(item);\r\n    return await Task.FromResult(result);\r\n}\n"})}),"\n",(0,t.jsxs)(n.p,{children:["\u274c ",(0,t.jsx)(n.strong,{children:"Don't replace tracer or logger factory during node execution:"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"public async Task<TOut> ExecuteAsync(TIn item, PipelineContext context, CancellationToken cancellationToken)\r\n{\r\n    // \u274c WRONG: Tracer is cached at node start\r\n    context.Tracer = new MyCustomTracer();\r\n    \r\n    // Rest of execution will use old tracer instance\r\n    return await ProcessAsync(item);\r\n}\n"})}),"\n",(0,t.jsx)(n.h3,{id:"what-you-should-do-instead",children:"What You Should Do Instead"}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Configure context before node execution starts:"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"// Configure everything upfront\r\nvar context = new PipelineContext();\r\ncontext.Items[PipelineContextKeys.GlobalRetryOptions] = new PipelineRetryOptions(3, 2, 5);\r\ncontext.LoggerFactory = new MyLoggerFactory();\r\ncontext.Tracer = new MyTracer();\r\n\r\n// Now start the pipeline - all configuration is fixed for the duration\r\nvar runner = new PipelineRunner();\r\nawait runner.RunAsync<MyPipeline>(context);\n"})}),"\n",(0,t.jsxs)(n.p,{children:["\u2705 ",(0,t.jsx)(n.strong,{children:"Modify context between pipeline runs if needed:"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:"// Pipeline 1 with configuration A\r\nawait runner.RunAsync<MyPipeline>(contextA);\r\n\r\n// Modify context for next run\r\ncontextB.Items[PipelineContextKeys.GlobalRetryOptions] = newOptions;\r\n\r\n// Pipeline 2 with configuration B\r\nawait runner.RunAsync<MyPipeline>(contextB);\n"})}),"\n",(0,t.jsx)(n.h3,{id:"why-this-matters",children:"Why This Matters"}),"\n",(0,t.jsx)(n.p,{children:"When context is cached at node scope (which execution strategies do automatically), mutations during execution can cause:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Inconsistent behavior"})," - Some items processed with old config, others with new config"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Unexpected retry behavior"})," - Items may retry with different policies mid-execution"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Tracing/logging gaps"})," - Tracer/logger changes don't apply to all items uniformly"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"debug-validation",children:"DEBUG Validation"}),"\n",(0,t.jsx)(n.p,{children:"In DEBUG builds, NPipeline detects mutations and throws a clear exception:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"InvalidOperationException: \r\n  Context immutability violation detected for node 'myNode': \r\n  Retry options were modified during node execution. \r\n  When using CachedNodeExecutionContext, context state must remain \r\n  immutable during node execution.\n"})}),"\n",(0,t.jsxs)(n.p,{children:["In RELEASE builds, this validation is ",(0,t.jsx)(n.strong,{children:"compiled out entirely"})," (zero overhead)."]}),"\n",(0,t.jsx)(n.h3,{id:"best-practices-summary",children:"Best Practices Summary"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Set all context configuration before starting pipeline execution"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Treat context as read-only during node execution"})}),"\n",(0,t.jsx)(n.li,{children:(0,t.jsx)(n.strong,{children:"Use separate context instances if you need different configurations"})}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Trust the automatic caching"})," - It's optimizing your pipeline for you"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["For architectural context, see ",(0,t.jsx)(n.a,{href:"/docs/architecture/execution-flow#context-immutability-during-execution",children:"Execution Flow: Context Immutability During Execution"}),"."]})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);