"use strict";(self.webpackChunknpipeline=self.webpackChunknpipeline||[]).push([[2508],{8453:(e,n,r)=>{r.d(n,{R:()=>o,x:()=>t});var s=r(6540);const i={},a=s.createContext(i);function o(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function t(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),s.createElement(a.Provider,{value:n},e.children)}},8497:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>t,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"advanced-topics/performance-hygiene","title":"Performance Hygiene","description":"Best practices for building high-performance, low-allocation data pipelines with NPipeline.","source":"@site/docs/advanced-topics/performance-hygiene.md","sourceDirName":"advanced-topics","slug":"/advanced-topics/performance-hygiene","permalink":"/docs/advanced-topics/performance-hygiene","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Performance Hygiene","description":"Best practices for building high-performance, low-allocation data pipelines with NPipeline.","sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"Advanced Topics","permalink":"/docs/advanced-topics"},"next":{"title":"Synchronous Fast Paths and ValueTask Optimization","permalink":"/docs/advanced-topics/synchronous-fast-paths"}}');var i=r(4848),a=r(8453);const o={title:"Performance Hygiene",description:"Best practices for building high-performance, low-allocation data pipelines with NPipeline.",sidebar_position:1},t="Performance Hygiene",l={},c=[{value:"1. Minimize Memory Allocations",id:"1-minimize-memory-allocations",level:2},{value:"Use <code>struct</code> for Small, Short-Lived Data",id:"use-struct-for-small-short-lived-data",level:3},{value:"Reuse Buffers",id:"reuse-buffers",level:3},{value:"2. Be Mindful of <code>async</code> and <code>await</code>",id:"2-be-mindful-of-async-and-await",level:2},{value:"Avoid <code>async</code> for Synchronous Work",id:"avoid-async-for-synchronous-work",level:3},{value:"Use <code>ValueTask&lt;T&gt;</code> for &quot;Fast Path&quot; Scenarios",id:"use-valuetaskt-for-fast-path-scenarios",level:3},{value:"3. Choose the Right Concurrency Strategy",id:"3-choose-the-right-concurrency-strategy",level:2},{value:"Advanced Parallel Configuration",id:"advanced-parallel-configuration",level:3},{value:"Queue Policy Options",id:"queue-policy-options",level:4},{value:"Output Buffer Control",id:"output-buffer-control",level:4},{value:"Ordering Considerations",id:"ordering-considerations",level:4},{value:"4. Streaming vs. Buffering",id:"4-streaming-vs-buffering",level:2},{value:"5. Use Benchmarking",id:"5-use-benchmarking",level:2}];function d(e){const n={a:"a",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"performance-hygiene",children:"Performance Hygiene"})}),"\n",(0,i.jsx)(n.p,{children:'NPipeline is designed for high performance, but building an efficient pipeline requires careful consideration of how you write your nodes and structure your data flow. "Performance hygiene" refers to the practice of writing code that is mindful of memory allocations, CPU usage, and data transfer overhead.'}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:["\u2139\ufe0f"," For specific optimization patterns and techniques, see ",(0,i.jsx)(n.a,{href:"/docs/advanced-topics/synchronous-fast-paths",children:"Synchronous Fast Paths"}),"."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"By following these best practices, you can ensure your pipelines run as fast and efficiently as possible."}),"\n",(0,i.jsx)(n.h2,{id:"1-minimize-memory-allocations",children:"1. Minimize Memory Allocations"}),"\n",(0,i.jsx)(n.p,{children:"In high-throughput data pipelines, memory allocation can become a major bottleneck. The .NET garbage collector (GC) is highly optimized, but frequent, large allocations can lead to GC pressure, causing pauses that hurt performance."}),"\n",(0,i.jsxs)(n.h3,{id:"use-struct-for-small-short-lived-data",children:["Use ",(0,i.jsx)(n.code,{children:"struct"})," for Small, Short-Lived Data"]}),"\n",(0,i.jsxs)(n.p,{children:["If you are passing small, simple data objects between nodes, consider using a ",(0,i.jsx)(n.code,{children:"struct"})," instead of a ",(0,i.jsx)(n.code,{children:"class"}),". Structs are value types and are allocated on the stack (in most cases), which avoids putting pressure on the GC."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Good for performance:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"// A struct is allocated on the stack, avoiding GC pressure.\r\npublic readonly struct Point\r\n{\r\n    public int X { get; }\r\n    public int Y { get; }\r\n\r\n    public Point(int x, int y)\r\n    {\r\n        X = x;\r\n        Y = y;\r\n    }\r\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Avoid for high-throughput data:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"// A class is allocated on the heap, creating work for the GC.\r\npublic class Point\r\n{\r\n    public int X { get; set; }\r\n    public int Y { get; set; }\r\n}\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Caveat:"})," Be mindful of the size of your structs. Large structs can lead to expensive copy operations. As a rule of thumb, structs are ideal for types that are small (e.g., under 16 bytes) and immutable."]}),"\n",(0,i.jsx)(n.h3,{id:"reuse-buffers",children:"Reuse Buffers"}),"\n",(0,i.jsx)(n.p,{children:"If your nodes process data in batches or chunks (e.g., reading from a network stream), reuse buffers instead of allocating a new one for each operation."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"// Zero-allocation buffer reuse with direct processing\r\npublic async IAsyncEnumerable<ReadOnlyMemory<byte>> ProcessStream(Stream stream, CancellationToken cancellationToken)\r\n{\r\n    // Rent a buffer from the ArrayPool to avoid allocations\r\n    var buffer = ArrayPool<byte>.Shared.Rent(8192);\r\n    try\r\n    {\r\n        int bytesRead;\r\n        while ((bytesRead = await stream.ReadAsync(buffer, 0, buffer.Length, cancellationToken)) > 0)\r\n        {\r\n            // Return a ReadOnlyMemory slice - no allocation!\r\n            yield return new ReadOnlyMemory<byte>(buffer, 0, bytesRead);\r\n        }\r\n    }\r\n    finally\r\n    {\r\n        // Return the buffer to the pool for reuse\r\n        ArrayPool<byte>.Shared.Return(buffer);\r\n    }\r\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:"For even better performance, process the data directly without yielding:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"// Process data inline for maximum performance\r\npublic async Task ProcessStreamInline(Stream stream, Func<ReadOnlyMemory<byte>, Task> processor, CancellationToken cancellationToken)\r\n{\r\n    var buffer = ArrayPool<byte>.Shared.Rent(8192);\r\n    try\r\n    {\r\n        int bytesRead;\r\n        while ((bytesRead = await stream.ReadAsync(buffer, 0, buffer.Length, cancellationToken)) > 0)\r\n        {\r\n            // Process immediately without yielding\r\n            await processor(new ReadOnlyMemory<byte>(buffer, 0, bytesRead));\r\n        }\r\n    }\r\n    finally\r\n    {\r\n        ArrayPool<byte>.Shared.Return(buffer);\r\n    }\r\n}\n"})}),"\n",(0,i.jsxs)(n.h2,{id:"2-be-mindful-of-async-and-await",children:["2. Be Mindful of ",(0,i.jsx)(n.code,{children:"async"})," and ",(0,i.jsx)(n.code,{children:"await"})]}),"\n",(0,i.jsxs)(n.p,{children:["While ",(0,i.jsx)(n.code,{children:"async/await"})," is essential for I/O-bound work, it does introduce some overhead."]}),"\n",(0,i.jsxs)(n.h3,{id:"avoid-async-for-synchronous-work",children:["Avoid ",(0,i.jsx)(n.code,{children:"async"})," for Synchronous Work"]}),"\n",(0,i.jsxs)(n.p,{children:["If a method doesn't perform any truly asynchronous operations, don't mark it as ",(0,i.jsx)(n.code,{children:"async"}),". You can return a completed ",(0,i.jsx)(n.code,{children:"Task"})," directly."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Good:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"public Task<int> GetConstantAsync()\r\n{\r\n    // No await needed, so no async state machine is generated.\r\n    return Task.FromResult(42);\r\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Avoid:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"public async Task<int> GetConstantAsync()\r\n{\r\n    // Unnecessary async/await creates overhead.\r\n    return await Task.FromResult(42);\r\n}\n"})}),"\n",(0,i.jsxs)(n.h3,{id:"use-valuetaskt-for-fast-path-scenarios",children:["Use ",(0,i.jsx)(n.code,{children:"ValueTask<T>"}),' for "Fast Path" Scenarios']}),"\n",(0,i.jsxs)(n.p,{children:["If your method is often able to return a result synchronously (e.g., from a cache), but may sometimes need to be asynchronous, use ",(0,i.jsx)(n.code,{children:"ValueTask<T>"}),". This avoids a heap allocation for the ",(0,i.jsx)(n.code,{children:"Task"})," object in the synchronous case."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"This is especially critical for transformer nodes in high-volume pipelines."})," Many transforms are synchronous or have a high synchronous fast path (cache hits, simple mappings). Using ",(0,i.jsx)(n.code,{children:"Task<T>"})," for these transforms creates millions of unnecessary heap allocations per second, causing constant GC pressure."]}),"\n",(0,i.jsxs)(n.p,{children:["For comprehensive implementation guidance, including critical constraints and real-world examples, see ",(0,i.jsx)(n.a,{href:"/docs/advanced-topics/synchronous-fast-paths",children:(0,i.jsx)(n.strong,{children:"Synchronous Fast Paths and ValueTask Optimization"})}),"\u2014the dedicated deep-dive guide that covers the complete implementation pattern, performance impact quantification, and dangerous constraints you must understand."]}),"\n",(0,i.jsx)(n.h2,{id:"3-choose-the-right-concurrency-strategy",children:"3. Choose the Right Concurrency Strategy"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"I/O-Bound Work:"})," For nodes that spend most of their time waiting for network or disk I/O, use the ",(0,i.jsx)(n.a,{href:"/docs/extensions/parallelism",children:"Parallelism Extension"})," with a relatively high ",(0,i.jsx)(n.code,{children:"MaxDegreeOfParallelism"}),". This ensures that while some tasks are waiting, others are actively being processed."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"CPU-Bound Work:"})," For nodes performing intensive calculations, set ",(0,i.jsx)(n.code,{children:"MaxDegreeOfParallelism"})," to a value close to ",(0,i.jsx)(n.code,{children:"Environment.ProcessorCount"}),". Note that this is already the default behavior when no value is specified, so you typically don't need to set it explicitly."]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"advanced-parallel-configuration",children:"Advanced Parallel Configuration"}),"\n",(0,i.jsxs)(n.p,{children:["The Parallelism Extension provides fine-grained control over execution behavior through the ",(0,i.jsx)(n.code,{children:"ParallelOptions"})," class:"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"using NPipeline.Extensions.Parallelism;\r\n\r\n// Configure parallel execution with advanced options\r\nvar parallelOptions = new ParallelOptions\r\n{\r\n    MaxDegreeOfParallelism = Environment.ProcessorCount, // Default when null\r\n    MaxQueueLength = 1000, // Controls backpressure by limiting input queue size\r\n    QueuePolicy = BoundedQueuePolicy.Block, // Behavior when queue is full\r\n    OutputBufferCapacity = 500, // Throttles workers by limiting output buffer\r\n    PreserveOrdering = true // Default: maintains input ordering in output\r\n};\r\n\r\n// Apply to a specific transform node\r\nbuilder.WithParallelOptions(transformHandle, parallelOptions);\n"})}),"\n",(0,i.jsx)(n.h4,{id:"queue-policy-options",children:"Queue Policy Options"}),"\n",(0,i.jsxs)(n.p,{children:["When ",(0,i.jsx)(n.code,{children:"MaxQueueLength"})," is set, you can control behavior when the queue becomes full:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"Block"})})," (default): The producer blocks until space is available, providing natural backpressure"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"DropNewest"})}),": Incoming items are discarded when the queue is full"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"DropOldest"})}),": The oldest items in the queue are discarded to make room for new ones"]}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"output-buffer-control",children:"Output Buffer Control"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.code,{children:"OutputBufferCapacity"})," option creates an additional throttling mechanism:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"When specified, it limits how many processed results can queue ahead of downstream consumption"}),"\n",(0,i.jsx)(n.li,{children:"This restores end-to-end backpressure when downstream nodes are slow"}),"\n",(0,i.jsx)(n.li,{children:"When null (default), output buffering is unbounded, which can lead to memory accumulation under sustained load"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"ordering-considerations",children:"Ordering Considerations"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"PreserveOrdering"})," flag (default: true) ensures output maintains input order"]}),"\n",(0,i.jsx)(n.li,{children:"Setting it to false can increase throughput but results in unordered output"}),"\n",(0,i.jsxs)(n.li,{children:["Note: Drop-policy paths (",(0,i.jsx)(n.code,{children:"DropNewest"}),", ",(0,i.jsx)(n.code,{children:"DropOldest"}),") are inherently unordered regardless of this setting"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"4-streaming-vs-buffering",children:"4. Streaming vs. Buffering"}),"\n",(0,i.jsx)(n.p,{children:"NPipeline is designed around a streaming-first philosophy. Nodes should process items as they arrive and yield results immediately. Avoid collecting all items from the input into a list before processing."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Good (Streaming):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"public async IAsyncEnumerable<string> ExecuteAsync(IAsyncEnumerable<string> input, CancellationToken cancellationToken)\r\n{\r\n    // Process items as they come in.\r\n    await foreach (var item in input.WithCancellation(cancellationToken))\r\n    {\r\n        yield return item.ToUpper();\r\n    }\r\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Avoid (Buffering):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"public async IAsyncEnumerable<string> ExecuteAsync(IAsyncEnumerable<string> input, CancellationToken cancellationToken)\r\n{\r\n    // This buffers the entire input into memory before processing.\r\n    // It can lead to high memory usage and delays the start of processing.\r\n    var allItems = await input.ToListAsync(cancellationToken);\r\n\r\n    foreach (var item in allItems)\r\n    {\r\n        yield return item.ToUpper();\r\n    }\r\n}\n"})}),"\n",(0,i.jsx)(n.p,{children:"Buffering is only appropriate if your logic requires access to the entire dataset at once (e.g., sorting or calculating a global aggregate)."}),"\n",(0,i.jsx)(n.h2,{id:"5-use-benchmarking",children:"5. Use Benchmarking"}),"\n",(0,i.jsxs)(n.p,{children:["The most reliable way to improve performance is to measure it. Use tools like ",(0,i.jsx)(n.a,{href:"https://benchmarkdotnet.org/",children:"BenchmarkDotNet"})," to write micro-benchmarks for your critical nodes. This allows you to test different implementations and configurations to see which performs best."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-csharp",children:"[MemoryDiagnoser] // Track memory allocations\r\npublic class MyTransformBenchmarks\r\n{\r\n    private MyTransform _transform;\r\n    private IAsyncEnumerable<string> _data;\r\n\r\n    [Params(100, 1000)]\r\n    public int N;\r\n\r\n    [GlobalSetup]\r\n    public void Setup()\r\n    {\r\n        _transform = new MyTransform();\r\n        _data = new EnumerableSourceNode<string>(Enumerable.Range(0, N).Select(i => i.ToString())).GetAsyncEnumerator();\r\n    }\r\n\r\n    [Benchmark]\r\n    public async Task Transform()\r\n    {\r\n        await foreach(var _ in _transform.ExecuteAsync(_data))\r\n        {\r\n            // Consume results\r\n        }\r\n    }\r\n}\n"})})]})}function u(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);