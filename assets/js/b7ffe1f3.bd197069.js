"use strict";(self.webpackChunknpipeline=self.webpackChunknpipeline||[]).push([[1461],{28453:(e,r,n)=>{n.d(r,{R:()=>l,x:()=>a});var i=n(96540);const s={},o=i.createContext(s);function l(e){const r=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function a(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),i.createElement(o.Provider,{value:r},e.children)}},59484:(e,r,n)=>{n.r(r),n.d(r,{assets:()=>t,contentTitle:()=>a,default:()=>u,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"connectors/csv","title":"CSV Connector","description":"Read from and write to Comma-Separated Values (CSV) files with NPipeline using the CSV connector.","source":"@site/docs/connectors/csv.md","sourceDirName":"connectors","slug":"/connectors/csv","permalink":"/docs/connectors/csv","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"CSV Connector","description":"Read from and write to Comma-Separated Values (CSV) files with NPipeline using the CSV connector.","sidebar_position":1},"sidebar":"docsSidebar","previous":{"title":"Connectors Overview","permalink":"/docs/connectors"},"next":{"title":"Storage Provider Interface","permalink":"/docs/connectors/storage-provider"}}');var s=n(74848),o=n(28453);const l={title:"CSV Connector",description:"Read from and write to Comma-Separated Values (CSV) files with NPipeline using the CSV connector.",sidebar_position:1},a="CSV Connector",t={},c=[{value:"Installation",id:"installation",level:2},{value:"Storage Abstraction Layer",id:"storage-abstraction-layer",level:2},{value:"StorageUri",id:"storageuri",level:3},{value:"IStorageResolver",id:"istorageresolver",level:3},{value:"<code>CsvSourceNode&lt;T&gt;</code>",id:"csvsourcenodet",level:2},{value:"Configuration",id:"configuration",level:3},{value:"Example: Reading a CSV File",id:"example-reading-a-csv-file",level:3},{value:"<code>CsvSinkNode&lt;T&gt;</code>",id:"csvsinknodet",level:2},{value:"Configuration",id:"configuration-1",level:3},{value:"Example: Writing to a CSV File",id:"example-writing-to-a-csv-file",level:3},{value:"Advanced Configuration",id:"advanced-configuration",level:2},{value:"Buffer Size Configuration",id:"buffer-size-configuration",level:3},{value:"Example: Using a custom delimiter and no header",id:"example-using-a-custom-delimiter-and-no-header",level:3},{value:"Example: Transforming and Writing to CSV",id:"example-transforming-and-writing-to-csv",level:3},{value:"Related Topics",id:"related-topics",level:2}];function d(e){const r={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(r.header,{children:(0,s.jsx)(r.h1,{id:"csv-connector",children:"CSV Connector"})}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsx)(r.code,{children:"NPipeline.Connectors.Csv"})," package provides specialized source and sink nodes for working with Comma-Separated Values (CSV) files. This allows you to easily integrate CSV data into your pipelines as an input source or an output destination."]}),"\n",(0,s.jsxs)(r.p,{children:["This connector uses the popular ",(0,s.jsx)(r.a,{href:"https://joshclose.github.io/CsvHelper/",children:"CsvHelper"})," library under the hood, so it is powerful and highly configurable."]}),"\n",(0,s.jsx)(r.h2,{id:"installation",children:"Installation"}),"\n",(0,s.jsxs)(r.p,{children:["To use the CSV connector, install the ",(0,s.jsx)(r.code,{children:"NPipeline.Connectors.Csv"})," NuGet package:"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-bash",children:"dotnet add package NPipeline.Connectors.Csv\n"})}),"\n",(0,s.jsxs)(r.p,{children:["For the core NPipeline package and other available extensions, see the ",(0,s.jsx)(r.a,{href:"/docs/getting-started/installation",children:"Installation Guide"}),"."]}),"\n",(0,s.jsx)(r.h2,{id:"storage-abstraction-layer",children:"Storage Abstraction Layer"}),"\n",(0,s.jsx)(r.p,{children:"The CSV connector uses NPipeline's storage abstraction layer, which provides a unified way to work with different storage systems. This layer allows you to work with local files, cloud storage (like S3 or Azure Blob), and other storage systems using the same API."}),"\n",(0,s.jsx)(r.h3,{id:"storageuri",children:"StorageUri"}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsx)(r.code,{children:"StorageUri"}),' class represents a normalized storage location URI. It supports both absolute URIs (e.g., "s3://bucket/key") and local file paths. For local files, use the ',(0,s.jsx)(r.code,{children:"StorageUri.FromFilePath()"})," method:"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:'// For local files\r\nvar localFileUri = StorageUri.FromFilePath("data/input.csv");\r\n\r\n// For absolute URIs (e.g., cloud storage)\r\nvar cloudUri = StorageUri.Parse("s3://my-bucket/path/to/file.csv");\n'})}),"\n",(0,s.jsx)(r.h3,{id:"istorageresolver",children:"IStorageResolver"}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsx)(r.code,{children:"IStorageResolver"})," interface is responsible for discovering and resolving storage providers capable of handling a given ",(0,s.jsx)(r.code,{children:"StorageUri"}),". You must provide a resolver to both ",(0,s.jsx)(r.code,{children:"CsvSourceNode"})," and ",(0,s.jsx)(r.code,{children:"CsvSinkNode"}),"."]}),"\n",(0,s.jsx)(r.p,{children:"To create a resolver for standard file system operations, use:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:"var resolver = StorageProviderFactory.CreateResolver().Resolver;\n"})}),"\n",(0,s.jsx)(r.p,{children:"You may need a custom resolver when:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Working with cloud storage systems (S3, Azure, etc.)"}),"\n",(0,s.jsx)(r.li,{children:"Using custom storage providers"}),"\n",(0,s.jsx)(r.li,{children:"Needing to override default provider selection"}),"\n"]}),"\n",(0,s.jsx)(r.h2,{id:"csvsourcenodet",children:(0,s.jsx)(r.code,{children:"CsvSourceNode<T>"})}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsx)(r.code,{children:"CsvSourceNode<T>"})," reads data from a CSV file and emits each row as an item of type ",(0,s.jsx)(r.code,{children:"T"}),"."]}),"\n",(0,s.jsx)(r.h3,{id:"configuration",children:"Configuration"}),"\n",(0,s.jsxs)(r.p,{children:["The constructor for ",(0,s.jsx)(r.code,{children:"CsvSourceNode<T>"})," takes the file path, a storage resolver, and optional configuration for parsing the CSV."]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:"public CsvSourceNode(\r\n    StorageUri uri,\r\n    IStorageResolver resolver,\r\n    CsvConfiguration? configuration = null,\r\n    Encoding? encoding = null)\n"})}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"uri"})}),": The ",(0,s.jsx)(r.code,{children:"StorageUri"})," representing the location of the CSV file. Use ",(0,s.jsx)(r.code,{children:'StorageUri.FromFilePath("path/to/file.csv")'})," for local files."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"resolver"})}),": The ",(0,s.jsx)(r.code,{children:"IStorageResolver"})," to resolve storage providers. Create one using ",(0,s.jsx)(r.code,{children:"StorageProviderFactory.CreateResolver().Resolver"})," for standard file system support."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"configuration"})}),": An optional ",(0,s.jsx)(r.code,{children:"CsvConfiguration"})," object to customize parsing (e.g., delimiter, culture, quoting)."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"encoding"})}),": An optional ",(0,s.jsx)(r.code,{children:"Encoding"})," for the file. Defaults to UTF-8."]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"example-reading-a-csv-file",children:"Example: Reading a CSV File"}),"\n",(0,s.jsxs)(r.p,{children:["Let's assume you have a ",(0,s.jsx)(r.code,{children:"users.csv"})," file:"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csv",children:"Id,Name,Email\r\n1,Alice,alice@example.com\r\n2,Bob,bob@example.com\n"})}),"\n",(0,s.jsx)(r.p,{children:"And a corresponding C# record:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:"public sealed record User(int Id, string Name, string Email);\n"})}),"\n",(0,s.jsx)(r.p,{children:"You can read this data into your pipeline as follows:"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:'using NPipeline;\r\nusing NPipeline.Connectors;\r\nusing NPipeline.Connectors.Csv;\r\nusing NPipeline.DataFlow.DataPipes;\r\nusing NPipeline.DataFlow;\r\nusing NPipeline.Execution;\r\nusing NPipeline.Nodes;\r\nusing NPipeline.Pipeline;\r\nusing NPipeline.Tracing;\r\n\r\npublic sealed record User(int Id, string Name, string Email);\r\n\r\npublic sealed class CsvReaderPipeline : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        var resolver = StorageProviderFactory.CreateResolver().Resolver;\r\n        var source = builder.AddSource("csv_source", new CsvSourceNode<User>(StorageUri.FromFilePath("users.csv"), resolver));\r\n        var sink = builder.AddSink<ConsoleSinkNode, User>("console_sink");\r\n\r\n        builder.Connect(source, sink);\r\n    }\r\n}\r\n\r\npublic sealed class ConsoleSinkNode : SinkNode<User>\r\n{\r\n    public override async Task ExecuteAsync(\r\n        IDataPipe<User> input,\r\n        PipelineContext context,\r\n        IPipelineActivity parentActivity,\r\n        CancellationToken cancellationToken)\r\n    {\r\n        await foreach (var user in input.WithCancellation(cancellationToken))\r\n        {\r\n            Console.WriteLine($"Received: {user}");\r\n        }\r\n    }\r\n}\r\n\r\npublic static class Program\r\n{\r\n    public static async Task Main(string[] args)\r\n    {\r\n        // Create a dummy CSV file for demonstration\r\n        await System.IO.File.WriteAllLinesAsync("users.csv", new[]\r\n        {\r\n            "Id,Name,Email",\r\n            "1,Alice,alice@example.com",\r\n            "2,Bob,bob@example.com"\r\n        });\r\n\r\n        // Create and run the pipeline\r\n        var runner = PipelineRunner.Create();\r\n        await runner.RunAsync<CsvReaderPipeline>();\r\n\r\n        Console.WriteLine("CSV reading completed");\r\n    }\r\n}\n'})}),"\n",(0,s.jsx)(r.p,{children:(0,s.jsx)(r.strong,{children:"Expected Output:"})}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-text",children:"Received: User { Id = 1, Name = Alice, Email = alice@example.com }\r\nReceived: User { Id = 2, Name = Bob, Email = bob@example.com }\r\nCSV reading completed\n"})}),"\n",(0,s.jsx)(r.h2,{id:"csvsinknodet",children:(0,s.jsx)(r.code,{children:"CsvSinkNode<T>"})}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsx)(r.code,{children:"CsvSinkNode<T>"})," writes items from the pipeline to a CSV file."]}),"\n",(0,s.jsx)(r.h3,{id:"configuration-1",children:"Configuration"}),"\n",(0,s.jsxs)(r.p,{children:["The constructor for ",(0,s.jsx)(r.code,{children:"CsvSinkNode<T>"})," takes the file path, a storage resolver, and optional configuration for writing the CSV."]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:"public CsvSinkNode(\r\n    StorageUri uri,\r\n    IStorageResolver resolver,\r\n    CsvConfiguration? configuration = null,\r\n    Encoding? encoding = null)\n"})}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"uri"})}),": The ",(0,s.jsx)(r.code,{children:"StorageUri"})," representing the location of the output CSV file. Use ",(0,s.jsx)(r.code,{children:'StorageUri.FromFilePath("path/to/file.csv")'})," for local files."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"resolver"})}),": The ",(0,s.jsx)(r.code,{children:"IStorageResolver"})," to resolve storage providers. Create one using ",(0,s.jsx)(r.code,{children:"StorageProviderFactory.CreateResolver().Resolver"})," for standard file system support."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"configuration"})}),": An optional ",(0,s.jsx)(r.code,{children:"CsvConfiguration"})," object to customize writing."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.code,{children:"encoding"})}),": An optional ",(0,s.jsx)(r.code,{children:"Encoding"})," for the file. Defaults to UTF-8."]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"example-writing-to-a-csv-file",children:"Example: Writing to a CSV File"}),"\n",(0,s.jsxs)(r.p,{children:["Let's take processed user data and write it to an ",(0,s.jsx)(r.code,{children:"output.csv"})," file."]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:'using NPipeline.Connectors;\r\nusing NPipeline.Connectors.Csv;\r\nusing NPipeline.Execution;\r\nusing NPipeline.Extensions.Testing;\r\nusing NPipeline.Nodes;\r\nusing NPipeline.Pipeline;\r\n\r\npublic sealed record ProcessedUser(int Id, string FullName, string Status);\r\n\r\npublic sealed class CsvWriterPipeline : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        var resolver = StorageProviderFactory.CreateResolver().Resolver;\r\n        var source = builder.AddSource<InMemorySourceNode<ProcessedUser>, ProcessedUser>("source");\r\n        var sink = builder.AddSink("csv_sink", new CsvSinkNode<ProcessedUser>(StorageUri.FromFilePath("output.csv"), resolver));\r\n\r\n        builder.Connect(source, sink);\r\n    }\r\n}\r\n\r\npublic static class Program\r\n{\r\n    public static async Task Main(string[] args)\r\n    {\r\n        var users = new List<ProcessedUser>\r\n        {\r\n            new(1, "Alice Smith", "Active"),\r\n            new(2, "Bob Johnson", "Inactive")\r\n        };\r\n\r\n        // Set up test data\r\n        var context = PipelineContext.Default;\r\n        context.Items[typeof(InMemorySourceNode<ProcessedUser>).FullName!] = users.ToArray();\r\n\r\n        var runner = PipelineRunner.Create();\r\n        await runner.RunAsync<CsvWriterPipeline>(context);\r\n\r\n        Console.WriteLine("\\nContent of output.csv:");\r\n        Console.WriteLine(await System.IO.File.ReadAllTextAsync("output.csv"));\r\n    }\r\n}\n'})}),"\n",(0,s.jsx)(r.p,{children:(0,s.jsxs)(r.strong,{children:["Expected ",(0,s.jsx)(r.code,{children:"output.csv"})," Content:"]})}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csv",children:"Id,FullName,Status\r\n1,Alice Smith,Active\r\n2,Bob Johnson,Inactive\n"})}),"\n",(0,s.jsx)(r.h2,{id:"advanced-configuration",children:"Advanced Configuration"}),"\n",(0,s.jsxs)(r.p,{children:["Both ",(0,s.jsx)(r.code,{children:"CsvSourceNode"})," and ",(0,s.jsx)(r.code,{children:"CsvSinkNode"})," accept an optional ",(0,s.jsx)(r.code,{children:"CsvConfiguration"})," object from the CsvHelper library in their constructors. This allows you to customize parsing and writing behavior."]}),"\n",(0,s.jsx)(r.p,{children:"Common configuration options include:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"HasHeaderRecord"}),": Specify whether the CSV file has a header row (default is ",(0,s.jsx)(r.code,{children:"true"}),")."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"Delimiter"}),": Change the field delimiter (e.g., to a tab ",(0,s.jsx)(r.code,{children:"\\t"})," or semicolon ",(0,s.jsx)(r.code,{children:";"}),")."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"CultureInfo"}),": Specify the culture to use for parsing numbers and dates."]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.code,{children:"BufferSize"}),": Controls the buffer size for the StreamWriter used in CSV operations (default is 1024)."]}),"\n"]}),"\n",(0,s.jsx)(r.h3,{id:"buffer-size-configuration",children:"Buffer Size Configuration"}),"\n",(0,s.jsxs)(r.p,{children:["The ",(0,s.jsx)(r.a,{href:"../../../src/NPipeline.Connectors.Csv/CsvConfiguration.cs:16",children:(0,s.jsx)(r.code,{children:"BufferSize"})})," property controls the internal buffer size for CSV I/O operations:"]}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Default value"}),": 1024 bytes"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Purpose"}),": Determines the size of the buffer used by StreamWriter when reading or writing CSV files"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Performance impact"}),": Larger buffers can improve I/O performance for large files but use more memory"]}),"\n"]}),"\n",(0,s.jsx)(r.p,{children:"When to adjust BufferSize:"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Increase"})," (e.g., 4096, 8192) for:","\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Processing very large CSV files"}),"\n",(0,s.jsx)(r.li,{children:"High-throughput scenarios where I/O performance is critical"}),"\n",(0,s.jsx)(r.li,{children:"Systems with abundant memory resources"}),"\n"]}),"\n"]}),"\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:"Decrease"})," (e.g., 512) for:","\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsx)(r.li,{children:"Memory-constrained environments"}),"\n",(0,s.jsx)(r.li,{children:"Processing many small CSV files concurrently"}),"\n",(0,s.jsx)(r.li,{children:"Scenarios where memory usage must be tightly controlled"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:'// Example: Custom buffer size for large file processing\r\nvar largeFileConfig = new CsvConfiguration()\r\n{\r\n    BufferSize = 8192, // 8KB buffer for better performance with large files\r\n    HelperConfiguration = {\r\n        Delimiter = ",",\r\n        HasHeaderRecord = true\r\n    }\r\n};\r\n\r\nvar resolver = StorageProviderFactory.CreateResolver().Resolver;\r\nvar source = new CsvSourceNode<User>(StorageUri.FromFilePath("large_dataset.csv"), resolver, largeFileConfig);\n'})}),"\n",(0,s.jsx)(r.h3,{id:"example-using-a-custom-delimiter-and-no-header",children:"Example: Using a custom delimiter and no header"}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:'using CsvHelper.Configuration;\r\nusing System.Globalization;\r\n\r\n// Configure for a tab-separated file with no header\r\nvar config = new CsvConfiguration(CultureInfo.InvariantCulture)\r\n{\r\n    Delimiter = "\\t",\r\n    HasHeaderRecord = false,\r\n};\r\n\r\n// The node will expect properties to be mapped by index\r\n// This requires using CsvHelper\'s class mapping feature\r\npublic sealed class UserMap : ClassMap<User>\r\n{\r\n    public UserMap()\r\n    {\r\n        Map(m => m.Id).Index(0);\r\n        Map(m => m.Name).Index(1);\r\n        Map(m => m.Email).Index(2);\r\n    }\r\n}\r\n\r\nvar resolver = StorageProviderFactory.CreateResolver().Resolver;\r\nvar source = new CsvSourceNode<User>(StorageUri.FromFilePath("users.tsv"), resolver, config);\n'})}),"\n",(0,s.jsxs)(r.p,{children:["In this advanced scenario, we configure the source to read a tab-separated file (",(0,s.jsx)(r.code,{children:".tsv"}),") that does not have a header. Because there's no header, we must provide a ",(0,s.jsx)(r.code,{children:"ClassMap"})," to tell CsvHelper how to map columns by their index to the properties of our ",(0,s.jsx)(r.code,{children:"User"})," record. You can register the class map through the ",(0,s.jsx)(r.code,{children:"CsvConfiguration"})," object."]}),"\n",(0,s.jsx)(r.h3,{id:"example-transforming-and-writing-to-csv",children:"Example: Transforming and Writing to CSV"}),"\n",(0,s.jsx)(r.p,{children:"This pipeline transforms user data and writes the result to a new CSV file."}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csharp",children:'using NPipeline.Connectors.Csv;\r\nusing NPipeline.Execution;\r\nusing NPipeline.Nodes;\r\nusing NPipeline.Pipeline;\r\n\r\npublic sealed record UserSummary(string Name, string Domain);\r\n\r\npublic sealed class Summarizer : TransformNode<User, UserSummary>\r\n{\r\n    public override Task<UserSummary> ExecuteAsync(\r\n        User item,\r\n        PipelineContext context,\r\n        CancellationToken cancellationToken)\r\n    {\r\n        var domain = item.Email.Split(\'@\')[1];\r\n        return Task.FromResult(new UserSummary(item.Name, domain));\r\n    }\r\n}\r\n\r\npublic sealed class CsvTransformPipeline : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        var resolver = StorageProviderFactory.CreateResolver().Resolver;\r\n        var source = builder.AddSource("csv_source", new CsvSourceNode<User>(StorageUri.FromFilePath("users.csv"), resolver));\r\n        var transform = builder.AddTransform<Summarizer, User, UserSummary>("summarizer");\r\n        var sink = builder.AddSink("csv_sink", new CsvSinkNode<UserSummary>(StorageUri.FromFilePath("summaries.csv"), resolver));\r\n\r\n        builder.Connect(source, transform);\r\n        builder.Connect(transform, sink);\r\n    }\r\n}\r\n\r\npublic class Program\r\n{\r\n    public static async Task Main(string[] args)\r\n    {\r\n        var runner = PipelineRunner.Create();\r\n        await runner.RunAsync<CsvTransformPipeline>();\r\n    }\r\n}\n'})}),"\n",(0,s.jsxs)(r.p,{children:["After running, this will create a ",(0,s.jsx)(r.code,{children:"summaries.csv"})," file with the following content:"]}),"\n",(0,s.jsx)(r.pre,{children:(0,s.jsx)(r.code,{className:"language-csv",children:"Name,Domain\r\nAlice,example.com\r\nBob,example.com\n"})}),"\n",(0,s.jsxs)(r.p,{children:["For more advanced configuration, refer to the ",(0,s.jsx)(r.a,{href:"https://joshclose.github.io/CsvHelper/getting-started/",children:"CsvHelper documentation"}),"."]}),"\n",(0,s.jsx)(r.h2,{id:"related-topics",children:"Related Topics"}),"\n",(0,s.jsxs)(r.ul,{children:["\n",(0,s.jsxs)(r.li,{children:[(0,s.jsx)(r.strong,{children:(0,s.jsx)(r.a,{href:"../.",children:"NPipeline Extensions Index"})}),": Return to the extensions overview."]}),"\n"]})]})}function u(e={}){const{wrapper:r}={...(0,o.R)(),...e.components};return r?(0,s.jsx)(r,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);