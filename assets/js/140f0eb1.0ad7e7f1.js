"use strict";(self.webpackChunknpipeline=self.webpackChunknpipeline||[]).push([[328],{7872:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>p,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"extensions/parallelism","title":"Parallelism","description":"Enhance your NPipeline\'s performance by leveraging parallel processing capabilities.","source":"@site/docs/extensions/parallelism.md","sourceDirName":"extensions","slug":"/extensions/parallelism","permalink":"/npipeline.dev/docs/extensions/parallelism","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"title":"Parallelism","description":"Enhance your NPipeline\'s performance by leveraging parallel processing capabilities.","sidebar_position":2},"sidebar":"docsSidebar","previous":{"title":"Dependency Injection","permalink":"/npipeline.dev/docs/extensions/dependency-injection"},"next":{"title":"Testing Extensions","permalink":"/npipeline.dev/docs/extensions/testing"}}');var l=r(4848),s=r(8453);const t={title:"Parallelism",description:"Enhance your NPipeline's performance by leveraging parallel processing capabilities.",sidebar_position:2},a="Parallelism",o={},c=[{value:"Understanding Parallelism in NPipeline",id:"understanding-parallelism-in-npipeline",level:2},{value:"<code>NPipeline.Extensions.Parallelism</code>",id:"npipelineextensionsparallelism",level:2},{value:"Example: Parallel Transform",id:"example-parallel-transform",level:3},{value:"Non-Ordered Parallel Execution for Maximum Throughput",id:"non-ordered-parallel-execution-for-maximum-throughput",level:2},{value:"Example: Non-Ordered Parallel Processing",id:"example-non-ordered-parallel-processing",level:3},{value:"When to Use Non-Ordered Execution",id:"when-to-use-non-ordered-execution",level:3},{value:"Trade-offs",id:"trade-offs",level:3},{value:"Advanced Parallel Options",id:"advanced-parallel-options",level:2},{value:"Queue Policies",id:"queue-policies",level:3},{value:"Considerations for Parallelism",id:"considerations-for-parallelism",level:2},{value:"\u2705 Best Practices",id:"white_check_mark-best-practices",level:2},{value:"\u27a1\ufe0f Next Steps",id:"arrow_right-next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,l.jsxs)(l.Fragment,{children:[(0,l.jsx)(n.header,{children:(0,l.jsx)(n.h1,{id:"parallelism",children:"Parallelism"})}),"\n",(0,l.jsxs)(n.p,{children:["NPipeline is designed for high performance, and a key aspect of this is its ability to execute parts of your pipeline in parallel. The ",(0,l.jsx)(n.a,{href:"../../../src/NPipeline.Extensions.Parallelism/NPipeline.Extensions.Parallelism.csproj",children:(0,l.jsx)(n.code,{children:"NPipeline.Extensions.Parallelism"})})," package provides tools and extensions to easily introduce parallel processing into your data flows, allowing you to scale out your computations and maximize throughput."]}),"\n",(0,l.jsx)(n.h2,{id:"understanding-parallelism-in-npipeline",children:"Understanding Parallelism in NPipeline"}),"\n",(0,l.jsx)(n.p,{children:"Parallelism in NPipeline typically means processing multiple data items concurrently, either within a single node or across multiple independent branches of a pipeline. This is distinct from concurrency, which is about managing multiple tasks that may or may not run simultaneously."}),"\n",(0,l.jsx)(n.h2,{id:"npipelineextensionsparallelism",children:(0,l.jsx)(n.a,{href:"../../../src/NPipeline.Extensions.Parallelism/NPipeline.Extensions.Parallelism.csproj",children:(0,l.jsx)(n.code,{children:"NPipeline.Extensions.Parallelism"})})}),"\n",(0,l.jsxs)(n.p,{children:["This extension package provides the ",(0,l.jsx)(n.a,{href:"../../../src/NPipeline.Extensions.Parallelism/ParallelExecutionStrategy.cs",children:(0,l.jsx)(n.code,{children:"ParallelExecutionStrategy"})})," class and builder extensions to enable and manage parallel execution."]}),"\n",(0,l.jsx)(n.h3,{id:"example-parallel-transform",children:"Example: Parallel Transform"}),"\n",(0,l.jsx)(n.p,{children:"Imagine you have a computationally intensive transformation that can be applied independently to each item. You can use parallel execution to process multiple items simultaneously."}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-csharp",children:'using NPipeline.Extensions.Parallelism;\r\nusing NPipeline.Extensions.Testing;\r\nusing NPipeline.Execution;\r\nusing NPipeline.Nodes;\r\n\r\npublic sealed class IntensiveTransform : TransformNode<int, int>\r\n{\r\n    public override async Task<int> ExecuteAsync(\r\n        int item,\r\n        PipelineContext context,\r\n        CancellationToken cancellationToken)\r\n    {\r\n        var logger = context.LoggerFactory.CreateLogger("IntensiveTransform");\r\n        logger.Log(NPipeline.Observability.Logging.LogLevel.Information,\r\n            $"Processing item {item} on Thread {Environment.CurrentManagedThreadId}");\r\n        await Task.Delay(100, cancellationToken); // Simulate intensive work\r\n        return item * 2;\r\n    }\r\n}\r\n\r\npublic sealed class ParallelPipeline : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        var source = builder.AddInMemorySource<int>();\r\n        var transform = builder.AddTransform<IntensiveTransform, int, int>();\r\n        var sink = builder.AddInMemorySink<int>();\r\n\r\n        builder.Connect(source, transform);\r\n        builder.Connect(transform, sink);\r\n\r\n        // Configure parallel execution for the transform\r\n        builder.WithParallelOptions(transform,\r\n            new ParallelOptions { MaxDegreeOfParallelism = 4 });\r\n\r\n        // Set the execution strategy to ParallelExecutionStrategy\r\n        transform.ExecutionStrategy = new ParallelExecutionStrategy();\r\n    }\r\n}\r\n\r\npublic static class Program\r\n{\r\n    public static async Task Main(string[] args)\r\n    {\r\n        // Set up test data\r\n        var testData = new[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\r\n        var context = new PipelineContext();\r\n        context.SetSourceData(testData);\r\n\r\n        Console.WriteLine("Starting parallel pipeline...");\r\n        var runner = new PipelineRunner();\r\n        await runner.RunAsync<ParallelPipeline>(context);\r\n        Console.WriteLine("Parallel pipeline finished.");\r\n    }\r\n}\n'})}),"\n",(0,l.jsx)(n.p,{children:"In this example, we:"}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsxs)(n.li,{children:["Use ",(0,l.jsx)(n.a,{href:"../../../src/NPipeline.Extensions.Parallelism/ParallelOptions.cs:47",children:(0,l.jsx)(n.code,{children:"WithParallelOptions(transform, new ParallelOptions { MaxDegreeOfParallelism = 4 })"})})," to configure parallel execution options for the transform"]}),"\n",(0,l.jsxs)(n.li,{children:["Set the ",(0,l.jsx)(n.a,{href:"../../../src/NPipeline/Graph/NodeDefinition.cs",children:(0,l.jsx)(n.code,{children:"ExecutionStrategy"})})," property to ",(0,l.jsx)(n.a,{href:"../../../src/NPipeline.Extensions.Parallelism/ParallelExecutionStrategy.cs",children:(0,l.jsx)(n.code,{children:"ParallelExecutionStrategy"})})," to enable parallel processing"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"non-ordered-parallel-execution-for-maximum-throughput",children:"Non-Ordered Parallel Execution for Maximum Throughput"}),"\n",(0,l.jsx)(n.p,{children:"By default, NPipeline preserves the order of items even when processing them in parallel. While this ensures predictable output, it can introduce overhead that reduces throughput. When the order of output items is not important for your use case, you can configure parallel execution to not preserve order, which can significantly increase throughput."}),"\n",(0,l.jsx)(n.h3,{id:"example-non-ordered-parallel-processing",children:"Example: Non-Ordered Parallel Processing"}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-csharp",children:'using NPipeline.Extensions.Parallelism;\r\nusing NPipeline.Extensions.Testing;\r\nusing NPipeline.Execution;\r\nusing NPipeline.Nodes;\r\nusing NPipeline.Pipeline;\r\n\r\npublic sealed class IntensiveTransform : TransformNode<int, int>\r\n{\r\n    public override async Task<int> ExecuteAsync(\r\n        int item,\r\n        PipelineContext context,\r\n        CancellationToken cancellationToken)\r\n    {\r\n        var logger = context.LoggerFactory.CreateLogger("IntensiveTransform");\r\n        logger.Log(NPipeline.Observability.Logging.LogLevel.Information,\r\n            $"Processing item {item} on Thread {Environment.CurrentManagedThreadId}");\r\n        await Task.Delay(new Random().Next(50, 150), cancellationToken); // Simulate variable work\r\n        return item * 2;\r\n    }\r\n}\r\n\r\npublic sealed class NonOrderedParallelPipeline : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        var source = builder.AddInMemorySource<int>();\r\n        var transform = builder.AddTransform<IntensiveTransform, int, int>();\r\n        var sink = builder.AddInMemorySink<int>();\r\n\r\n        builder.Connect(source, transform);\r\n        builder.Connect(transform, sink);\r\n\r\n        // Configure parallel execution with non-ordered output for maximum throughput\r\n        builder.WithParallelOptions(transform,\r\n            new ParallelOptions\r\n            {\r\n                MaxDegreeOfParallelism = 4,\r\n                PreserveOrdering = false  // Disable ordering to maximize throughput\r\n            });\r\n\r\n        // Set the execution strategy to ParallelExecutionStrategy\r\n        transform.ExecutionStrategy = new ParallelExecutionStrategy();\r\n    }\r\n}\r\n\r\npublic static class Program\r\n{\r\n    public static async Task Main(string[] args)\r\n    {\r\n        // Set up test data\r\n        var testData = new[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\r\n        var context = new PipelineContext();\r\n        context.SetSourceData(testData);\r\n\r\n        Console.WriteLine("Starting non-ordered parallel pipeline...");\r\n        var runner = new PipelineRunner();\r\n        await runner.RunAsync<NonOrderedParallelPipeline>(context);\r\n        Console.WriteLine("Non-ordered parallel pipeline finished.");\r\n    }\r\n}\n'})}),"\n",(0,l.jsxs)(n.p,{children:["In this example, we explicitly set ",(0,l.jsx)(n.code,{children:"PreserveOrdering = false"})," in the ",(0,l.jsx)(n.a,{href:"../../../src/NPipeline.Extensions.Parallelism/ParallelOptions.cs:34",children:(0,l.jsx)(n.code,{children:"ParallelOptions"})}),". This configuration:"]}),"\n",(0,l.jsxs)(n.ol,{children:["\n",(0,l.jsx)(n.li,{children:"Allows items to be emitted as soon as they are processed, without waiting for slower items"}),"\n",(0,l.jsx)(n.li,{children:"Eliminates the overhead of tracking and reordering items"}),"\n",(0,l.jsx)(n.li,{children:"Can significantly increase throughput, especially when processing times vary widely"}),"\n",(0,l.jsx)(n.li,{children:"Results in output that may not match the input order"}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"when-to-use-non-ordered-execution",children:"When to Use Non-Ordered Execution"}),"\n",(0,l.jsxs)(n.p,{children:["Consider using ",(0,l.jsx)(n.code,{children:"PreserveOrdering = false"})," when:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Order is irrelevant"}),": Your downstream processing doesn't depend on the input order"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Maximum throughput is critical"}),": You need to process as many items as possible per unit of time"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"Processing times vary significantly"}),": Some items take much longer to process than others"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:"You're aggregating results"}),": You're collecting statistics or aggregating data where order doesn't matter"]}),"\n"]}),"\n",(0,l.jsx)(n.h3,{id:"trade-offs",children:"Trade-offs"}),"\n",(0,l.jsxs)(n.table,{children:[(0,l.jsx)(n.thead,{children:(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.th,{children:"Aspect"}),(0,l.jsx)(n.th,{children:"PreserveOrdering: true (Default)"}),(0,l.jsx)(n.th,{children:"PreserveOrdering: false"})]})}),(0,l.jsxs)(n.tbody,{children:[(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Throughput"})}),(0,l.jsx)(n.td,{children:"Good"}),(0,l.jsx)(n.td,{children:"Excellent"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Output Order"})}),(0,l.jsx)(n.td,{children:"Matches input order"}),(0,l.jsx)(n.td,{children:"May be out of order"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Memory Usage"})}),(0,l.jsx)(n.td,{children:"Higher (needs to buffer)"}),(0,l.jsx)(n.td,{children:"Lower"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Latency"})}),(0,l.jsx)(n.td,{children:"Higher (waits for slow items)"}),(0,l.jsx)(n.td,{children:"Lower (emits immediately)"})]}),(0,l.jsxs)(n.tr,{children:[(0,l.jsx)(n.td,{children:(0,l.jsx)(n.strong,{children:"Use Case"})}),(0,l.jsx)(n.td,{children:"Order-sensitive processing"}),(0,l.jsx)(n.td,{children:"Maximum throughput scenarios"})]})]})]}),"\n",(0,l.jsx)(n.h2,{id:"advanced-parallel-options",children:"Advanced Parallel Options"}),"\n",(0,l.jsxs)(n.p,{children:["The ",(0,l.jsx)(n.a,{href:"../../../src/NPipeline.Extensions.Parallelism/ParallelOptions.cs:34",children:(0,l.jsx)(n.code,{children:"ParallelOptions"})})," class provides additional configuration options for fine-tuning parallel execution:"]}),"\n",(0,l.jsx)(n.pre,{children:(0,l.jsx)(n.code,{className:"language-csharp",children:"var options = new ParallelOptions\r\n{\r\n    MaxDegreeOfParallelism = 8,           // Maximum concurrent operations\r\n    MaxQueueLength = 1000,               // Bounded input queue for backpressure\r\n    QueuePolicy = BoundedQueuePolicy.Block, // Behavior when queue is full\r\n    OutputBufferCapacity = 500,           // Bounded output buffer\r\n    PreserveOrdering = true               // Whether to preserve input order\r\n};\n"})}),"\n",(0,l.jsx)(n.h3,{id:"queue-policies",children:"Queue Policies"}),"\n",(0,l.jsxs)(n.p,{children:["When ",(0,l.jsx)(n.code,{children:"MaxQueueLength"})," is specified, you can control the behavior when the queue reaches its capacity:"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.code,{children:"BoundedQueuePolicy.Block"})}),": Wait until space becomes available (default)"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.code,{children:"BoundedQueuePolicy.DropNewest"})}),": Drop the incoming item"]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.code,{children:"BoundedQueuePolicy.DropOldest"})}),": Remove the oldest item to make space"]}),"\n"]}),"\n",(0,l.jsx)(n.h2,{id:"considerations-for-parallelism",children:"Considerations for Parallelism"}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Degree of Parallelism:"})," Carefully choose the ",(0,l.jsx)(n.code,{children:"MaxDegreeOfParallelism"}),". Too high a value can lead to excessive resource consumption (CPU, memory, threads) and diminish returns due to context switching overhead. Too low a value might underutilize available resources."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Thread Safety:"})," Ensure that any shared state or external resources accessed by your parallel nodes are thread-safe. If your nodes are pure functions (operating only on their input and producing output without side effects), this is less of a concern."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Order Preservation:"})," By default, NPipeline maintains the order of items even when processing them in parallel. If order is not critical and you need maximum throughput, you can configure nodes to not preserve order by setting ",(0,l.jsx)(n.code,{children:"PreserveOrdering = false"}),"."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Resource Contention:"})," Be aware of potential bottlenecks when multiple parallel tasks try to access the same limited resource (e.g., a single database connection, a slow API)."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Debugging:"})," Debugging parallel code can be more complex. Ensure you have good logging and monitoring in place."]}),"\n"]}),"\n"]}),"\n",(0,l.jsxs)(n.h2,{id:"white_check_mark-best-practices",children:["\u2705"," Best Practices"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Identify Parallelizable Work:"})," Apply parallelism to parts of your pipeline where operations are independent and computationally intensive."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Start Small:"})," Begin with a low degree of parallelism and incrementally increase it while monitoring performance metrics (CPU, memory, throughput) to find the optimal balance."]}),"\n"]}),"\n",(0,l.jsxs)(n.li,{children:["\n",(0,l.jsxs)(n.p,{children:[(0,l.jsx)(n.strong,{children:"Profile:"})," Use profiling tools to identify bottlenecks and ensure that parallelism is indeed improving performance."]}),"\n"]}),"\n"]}),"\n",(0,l.jsx)(n.p,{children:"By strategically applying parallelism, you can significantly boost the processing capabilities of your NPipelines for demanding workloads."}),"\n",(0,l.jsxs)(n.h2,{id:"arrow_right-next-steps",children:["\u27a1\ufe0f"," Next Steps"]}),"\n",(0,l.jsxs)(n.ul,{children:["\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.a,{href:"/npipeline.dev/docs/extensions/dependency-injection",children:"Dependency Injection"})}),": Learn how to integrate NPipeline with dependency injection frameworks."]}),"\n",(0,l.jsxs)(n.li,{children:[(0,l.jsx)(n.strong,{children:(0,l.jsx)(n.a,{href:"/npipeline.dev/docs/extensions/testing",children:"Testing Pipelines"})}),": Understand how to effectively test your parallel pipelines."]}),"\n"]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,l.jsx)(n,{...e,children:(0,l.jsx)(d,{...e})}):d(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>t,x:()=>a});var i=r(6540);const l={},s=i.createContext(l);function t(e){const n=i.useContext(s);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(l):e.components||l:t(e.components),i.createElement(s.Provider,{value:n},e.children)}}}]);