"use strict";(self.webpackChunknpipeline=self.webpackChunknpipeline||[]).push([[4197],{8204:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>c,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"architecture/optimization-principles","title":"Optimization Principles - How NPipeline Achieves High Performance","description":"Deep dive into architectural and design decisions that enable NPipeline\'s exceptional performance characteristics.","source":"@site/docs/architecture/optimization-principles.md","sourceDirName":"architecture","slug":"/architecture/optimization-principles","permalink":"/docs/architecture/optimization-principles","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":13,"frontMatter":{"title":"Optimization Principles - How NPipeline Achieves High Performance","description":"Deep dive into architectural and design decisions that enable NPipeline\'s exceptional performance characteristics.","sidebar_position":13},"sidebar":"docsSidebar","previous":{"title":"Design Principles","permalink":"/docs/architecture/design-principles"},"next":{"title":"NPipeline Extensions","permalink":"/docs/extensions"}}');var s=i(4848),t=i(8453);const c={title:"Optimization Principles - How NPipeline Achieves High Performance",description:"Deep dive into architectural and design decisions that enable NPipeline's exceptional performance characteristics.",sidebar_position:13},l=void 0,a={},o=[{value:"Optimization Principles: How NPipeline Achieves High Performance",id:"optimization-principles-how-npipeline-achieves-high-performance",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"The Performance Challenge",id:"the-performance-challenge",level:2},{value:"Core Optimization Principles",id:"core-optimization-principles",level:2},{value:"1. Plan-Based Execution Model",id:"1-plan-based-execution-model",level:3},{value:"2. Zero Reflection During Steady State",id:"2-zero-reflection-during-steady-state",level:3},{value:"3. ICountable Optimization for Memory Efficiency",id:"3-icountable-optimization-for-memory-efficiency",level:3},{value:"4. Streaming-First Design with Lazy Evaluation",id:"4-streaming-first-design-with-lazy-evaluation",level:3},{value:"5. Allocation Reduction and ValueTask Optimization",id:"5-allocation-reduction-and-valuetask-optimization",level:3},{value:"6. Graph-Based Execution for Dependency Clarity",id:"6-graph-based-execution-for-dependency-clarity",level:3},{value:"7. Memory Layout and Cache Efficiency",id:"7-memory-layout-and-cache-efficiency",level:3},{value:"How These Principles Work Together",id:"how-these-principles-work-together",level:2},{value:"Measurable Results",id:"measurable-results",level:2},{value:"Trade-offs and When These Optimizations Matter",id:"trade-offs-and-when-these-optimizations-matter",level:2},{value:"When Optimization Matters Most",id:"when-optimization-matters-most",level:3},{value:"When Optimization Matters Less",id:"when-optimization-matters-less",level:3},{value:"See Also",id:"see-also",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h2,{id:"optimization-principles-how-npipeline-achieves-high-performance",children:"Optimization Principles: How NPipeline Achieves High Performance"}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsx)(n.p,{children:"Before understanding optimization principles, you should be familiar with:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/core-concepts",children:"Core Concepts Overview"})," - Basic NPipeline concepts and terminology"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/",children:"Architecture Overview"})," - Understanding NPipeline's internal architecture"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/core-concepts/pipeline-execution/execution-strategies",children:"Execution Strategies"})," - How nodes execute data"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["NPipeline's performance advantages don't come by accident. They're result of deliberate architectural decisions made at framework level. This document explains ",(0,s.jsx)(n.strong,{children:"why"})," behind NPipeline's design and how these choices combine to deliver measurable performance benefits."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"the-performance-challenge",children:"The Performance Challenge"}),"\n",(0,s.jsx)(n.p,{children:"Data processing frameworks face inherent tradeoffs:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flexibility"})," (supporting diverse use cases) vs. ",(0,s.jsx)(n.strong,{children:"Optimization"})," (pre-computing for specific scenarios)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Developer Experience"})," (intuitive APIs, reduced boilerplate) vs. ",(0,s.jsx)(n.strong,{children:"Performance"})," (minimal overhead, zero-cost abstractions)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Safety"})," (preventing errors) vs. ",(0,s.jsx)(n.strong,{children:"Speed"})," (avoiding runtime checks)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Most frameworks compromise by making reasonable defaults but allowing flexibility. NPipeline takes a different approach: optimize for most common, highest-impact scenarios while maintaining flexibility for others."}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"core-optimization-principles",children:"Core Optimization Principles"}),"\n",(0,s.jsx)(n.h3,{id:"1-plan-based-execution-model",children:"1. Plan-Based Execution Model"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Pre-compute everything that doesn't change per-item."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Traditional Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Interpreted: evaluate routing/execution strategy for every item\r\nforeach (var item in items)\r\n{\r\n    var strategy = DetermineStrategy(item);\r\n    var result = ExecuteStrategy(strategy, item);\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Compiled: determine execution plan once, execute same plan for all items\r\nvar executionPlan = CompileExecutionPlan(pipeline);\r\nforeach (var item in items)\r\n{\r\n    executionPlan.Execute(item);\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Eliminating per-item branching reduces CPU cache misses"}),"\n",(0,s.jsx)(n.li,{children:"Predictable instruction patterns improve branch prediction"}),"\n",(0,s.jsx)(n.li,{children:"The CPU pipeline can optimize hot path more effectively"}),"\n",(0,s.jsx)(n.li,{children:"In high-throughput scenarios: thousands of decisions per second become zero decisions"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact:"})," Measurable CPU efficiency improvement, especially on modern CPUs with deep pipelines."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"2-zero-reflection-during-steady-state",children:"2. Zero Reflection During Steady State"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Pay reflection cost upfront, then never again."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Reflection Overhead:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Runtime type introspection (method resolution, property access)"}),"\n",(0,s.jsx)(n.li,{children:"Dynamic method invocation via delegates"}),"\n",(0,s.jsx)(n.li,{children:"Argument marshalling and unmarshalling"}),"\n",(0,s.jsx)(n.li,{children:"GC pressure from temporary objects created during reflection"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Traditional Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Per-item reflection: look up methods, invoke dynamically\r\nforeach (var item in items)\r\n{\r\n    var method = GetMethod(item.GetType());  // \u2190 Reflection\r\n    method.Invoke(transform, new[] { item }); // \u2190 Dynamic dispatch\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Compile-time: pre-compiled delegates to actual methods\r\nvar compiledDelegate = CompileDelegate<T>(method);\r\nforeach (var item in items)\r\n{\r\n    compiledDelegate(item);  // \u2190 Direct, compiled dispatch\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Reflection is expensive: 100-1000x slower than direct method calls"}),"\n",(0,s.jsx)(n.li,{children:"Pre-compiled delegates are statically typed, JIT-optimizable"}),"\n",(0,s.jsx)(n.li,{children:"Reflection GC pressure is eliminated during steady state"}),"\n",(0,s.jsx)(n.li,{children:"The JIT compiler can inline delegate calls"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact:"})," Particularly noticeable in scenarios with millions of items, where per-item reflection overhead becomes dominant cost."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"3-icountable-optimization-for-memory-efficiency",children:"3. ICountable Optimization for Memory Efficiency"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Know the size of your data upfront to make smart allocation decisions."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Problem:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"IEnumerable<T>"})," has no size information"]}),"\n",(0,s.jsx)(n.li,{children:"Buffers must be over-allocated or reallocated (expensive)"}),"\n",(0,s.jsx)(n.li,{children:"Collections often allocate larger capacity than needed (wasting memory)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline's Solution - ICountable:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"public interface ICountable\r\n{\r\n    long Count { get; }\r\n}\n"})}),"\n",(0,s.jsxs)(n.p,{children:["Pipes and collections implementing ",(0,s.jsx)(n.code,{children:"ICountable"})," expose their size, enabling:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Allocate exactly the right buffer size, no overshooting\r\nif (input is ICountable countable)\r\n{\r\n    var buffer = new T[countable.Count];\r\n    // Fill buffer with no reallocation\r\n}\r\nelse\r\n{\r\n    // Fall back to dynamic resizing for unknown sizes\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Right-sized buffers = reduced memory waste"}),"\n",(0,s.jsx)(n.li,{children:"Fewer reallocations = reduced allocation pressure"}),"\n",(0,s.jsx)(n.li,{children:"Predictable memory usage = easier capacity planning"}),"\n",(0,s.jsx)(n.li,{children:"Smaller GC working set = better cache locality"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact:"})," Especially important for pipelines with large intermediate collections (batching, aggregation)."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"4-streaming-first-design-with-lazy-evaluation",children:"4. Streaming-First Design with Lazy Evaluation"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Process data incrementally, never buffer unnecessarily."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Traditional Batch Processing Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Load entire dataset \u2192 Filter \u2192 Transform \u2192 Load output\r\n\u2193\r\nMemory usage = entire dataset in memory at once\r\n\u2193\r\nLarge GC pauses when data is collected\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline Streaming Approach:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Item 1 \u2192 Filter \u2192 Transform \u2192 Output\r\nItem 2 \u2192 Filter \u2192 Transform \u2192 Output\r\nItem 3 \u2192 Filter \u2192 Transform \u2192 Output\r\n\u2193\r\nMemory usage = only current item + accumulated state\r\n\u2193\r\nTiny, predictable GC pauses\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Implementation:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"IAsyncEnumerable<T>"})," for lazy evaluation"]}),"\n",(0,s.jsx)(n.li,{children:"Pull-based data flow (demand-driven)"}),"\n",(0,s.jsx)(n.li,{children:"State is only accumulated when explicitly required (aggregation, joins)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Memory usage scales with state complexity, not data volume"}),"\n",(0,s.jsx)(n.li,{children:"GC pause times are predictable and minimal"}),"\n",(0,s.jsx)(n.li,{children:"Latency for processing first item is low (no waiting for batch assembly)"}),"\n",(0,s.jsx)(n.li,{children:"Natural backpressure: slow consumers slow down producers"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Impact:"})," Enables processing of datasets far larger than available memory, with minimal latency impact."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"5-allocation-reduction-and-valuetask-optimization",children:"5. Allocation Reduction and ValueTask Optimization"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Eliminate unnecessary heap allocations in hot paths."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Problem - Traditional Async Framework:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Every transform returns Task<T>, even if work is synchronous\r\npublic async Task<OutputType> Transform(InputType item)\r\n{\r\n    if (cache.TryGetValue(item.Id, out var cached))\r\n    {\r\n        return cached;  // \u2190 Still allocates Task<T> on heap!\r\n    }\r\n    return await expensiveAsync();\r\n}\n"})}),"\n",(0,s.jsxs)(n.p,{children:["In a pipeline processing 1M items/sec with 90% cache hits: ",(0,s.jsx)(n.strong,{children:"900,000 Task allocations per second"})]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"NPipeline Approach - ValueTask:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// Check synchronous fast path first\r\npublic ValueTask<OutputType> Transform(InputType item)\r\n{\r\n    if (cache.TryGetValue(item.Id, out var cached))\r\n    {\r\n        return new ValueTask<OutputType>(cached);  // \u2190 Stack allocation, no GC\r\n    }\r\n    return new ValueTask<OutputType>(ExpensiveAsync());\r\n}\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why This Matters:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"ValueTask<T>"})," is a struct (stack-allocated)"]}),"\n",(0,s.jsx)(n.li,{children:"For synchronous results: zero heap allocations"}),"\n",(0,s.jsxs)(n.li,{children:["For asynchronous results: seamlessly transitions to ",(0,s.jsx)(n.code,{children:"Task<T>"})]}),"\n",(0,s.jsx)(n.li,{children:"No performance penalty for async fallback path"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Measured Impact:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Up to 90% reduction in GC pressure"})," in high-cache-hit scenarios"]}),"\n",(0,s.jsx)(n.li,{children:"Smoother throughput: fewer GC pauses"}),"\n",(0,s.jsx)(n.li,{children:'More predictable latency: less "garbage spikes"'}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Common Scenarios:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Data validation (usually synchronous)"}),"\n",(0,s.jsx)(n.li,{children:"Filtering (usually synchronous)"}),"\n",(0,s.jsx)(n.li,{children:"Cached enrichment (high synchronous fast path rate)"}),"\n",(0,s.jsx)(n.li,{children:"These represent everyday pipeline tasks, not edge cases"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"6-graph-based-execution-for-dependency-clarity",children:"6. Graph-Based Execution for Dependency Clarity"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Make execution flow explicit and optimizable."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Why a Graph?"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Clarity:"})," Visual representation of data dependencies"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Optimization:"})," Can identify parallelizable segments"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Composability:"})," Nodes can be chained, reused, tested independently"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Debuggability:"})," Clear data provenance (lineage)"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Execution Strategy:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:'The graph is traversed once during the "plan compilation" phase'}),"\n",(0,s.jsx)(n.li,{children:"Execution strategy (sequential, parallel) is determined from graph structure"}),"\n",(0,s.jsx)(n.li,{children:"No per-item graph traversal overhead"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h3,{id:"7-memory-layout-and-cache-efficiency",children:"7. Memory Layout and Cache Efficiency"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"The Principle:"})," Respect CPU cache behavior."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Key Decisions:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Value types for small data:"})," Structs with < 16 bytes avoid GC overhead"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Array-backed collections:"})," Better cache locality than linked lists"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Contiguous buffers:"})," CPU prefetcher can predict access patterns"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Minimize indirection:"})," Reduce pointer chasing in hot paths"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Example:"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"// \u2713 GOOD: Value types, contiguous memory\r\npublic readonly struct Event\r\n{\r\n    public long Timestamp { get; }\r\n    public int Value { get; }\r\n}\r\n\r\n// \u274c LESS OPTIMAL: Reference types, scattered memory\r\npublic class Event\r\n{\r\n    public long Timestamp { get; set; }\r\n    public int Value { get; set; }\r\n}\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"how-these-principles-work-together",children:"How These Principles Work Together"}),"\n",(0,s.jsx)(n.p,{children:"The principles don't operate in isolation; they combine synergistically:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"Plan-Based Execution\r\n  \u2193 Eliminates per-item decisions\r\n  \u251c\u2192 Enables JIT optimization\r\n  \u2514\u2192 Improves CPU cache behavior\r\n\r\nZero Reflection at Runtime\r\n  \u2193 Direct method dispatch\r\n  \u251c\u2192 Inlinable and optimizable by JIT\r\n  \u2514\u2192 Reduces memory allocations\r\n\r\nValueTask Optimization\r\n  \u2193 Eliminates allocations in fast paths\r\n  \u251c\u2192 Reduces GC pressure\r\n  \u2514\u2192 Smaller GC working set = better cache locality\r\n\r\nStreaming + Lazy Evaluation\r\n  \u2193 Process incrementally\r\n  \u251c\u2192 Predictable memory usage\r\n  \u2514\u2192 Minimal GC pauses\r\n\r\nICountable for Right-Sizing\r\n  \u2193 Allocate exactly what's needed\r\n  \u251c\u2192 Fewer reallocations\r\n  \u2514\u2192 Better memory cache behavior\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"measurable-results",children:"Measurable Results"}),"\n",(0,s.jsx)(n.p,{children:"The combination of these principles produces observable performance characteristics:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Metric"}),(0,s.jsx)(n.th,{children:"Typical Benefit"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"GC Pause Duration"})}),(0,s.jsx)(n.td,{children:"50-80% reduction vs. naive async approach"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Memory Allocations"})}),(0,s.jsx)(n.td,{children:"Up to 90% fewer in cache-hit scenarios"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Throughput (items/sec)"})}),(0,s.jsx)(n.td,{children:"2-5x improvement vs. interpreted frameworks"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Latency (p99)"})}),(0,s.jsx)(n.td,{children:"More predictable, fewer spikes"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"CPU Efficiency"})}),(0,s.jsx)(n.td,{children:"Better branch prediction, cache locality"})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"trade-offs-and-when-these-optimizations-matter",children:"Trade-offs and When These Optimizations Matter"}),"\n",(0,s.jsx)(n.h3,{id:"when-optimization-matters-most",children:"When Optimization Matters Most"}),"\n",(0,s.jsx)(n.p,{children:"These optimizations provide most benefit in:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"High-throughput scenarios:"})," Millions of items per second"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Multi-tenant systems:"})," GC pauses directly impact other tenants"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Real-time processing:"})," Latency spikes from GC pauses are unacceptable"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Long-running processes:"})," Accumulated allocation pressure matters"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency-sensitive workloads:"})," Predictable performance critical"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"when-optimization-matters-less",children:"When Optimization Matters Less"}),"\n",(0,s.jsx)(n.p,{children:"These optimizations have minimal impact if:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Throughput is low:"})," (< 1000 items/sec) - bottleneck is elsewhere"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Items are large:"})," (> 100KB) - allocation cost is tiny vs. processing cost"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Processing is CPU-bound:"})," GC pressure is secondary concern"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Latency spikes are acceptable:"})," SLAs allow for GC pauses"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"see-also",children:"See Also"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/advanced-topics/performance-hygiene",children:"Performance Hygiene"})," - Best practices for writing performant NPipeline code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/advanced-topics/synchronous-fast-paths",children:"Synchronous Fast Paths"})," - Master ValueTask patterns in your transform nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/component-architecture",children:"Component Architecture"})," - Understand how these principles are implemented in codebase"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/execution-flow",children:"Execution Flow"})," - How optimization principles affect data flow"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/performance-characteristics",children:"Performance Characteristics"})," - Measurable performance implications"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/core-concepts",children:"Architecture: Core Concepts"})," - Fundamental architectural building blocks"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"/docs/advanced-topics/performance-hygiene",children:"Performance Hygiene"}),":"]})," Best practices for writing performant NPipeline code"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"/docs/advanced-topics/synchronous-fast-paths",children:"Synchronous Fast Paths"}),":"]})," Master ValueTask patterns in your transform nodes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/component-architecture",children:"Component Architecture"}),":"]})," Understand how these principles are implemented in codebase"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsxs)(n.strong,{children:[(0,s.jsx)(n.a,{href:"/docs/architecture/performance-characteristics",children:"Performance Characteristics"}),":"]})," Understanding performance implications of different approaches"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>c,x:()=>l});var r=i(6540);const s={},t=r.createContext(s);function c(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:c(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);