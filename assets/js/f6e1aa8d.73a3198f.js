"use strict";(self.webpackChunknpipeline=self.webpackChunknpipeline||[]).push([[8307],{5489:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>r,toc:()=>o});const r=JSON.parse('{"id":"core-concepts/streaming-vs-buffering","title":"Streaming vs. Buffering","description":"Understand the differences between streaming and buffering in NPipeline and how they impact performance and memory usage.","source":"@site/docs/core-concepts/streaming-vs-buffering.md","sourceDirName":"core-concepts","slug":"/core-concepts/streaming-vs-buffering","permalink":"/docs/core-concepts/streaming-vs-buffering","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":8,"frontMatter":{"title":"Streaming vs. Buffering","description":"Understand the differences between streaming and buffering in NPipeline and how they impact performance and memory usage.","sidebar_position":8},"sidebar":"docsSidebar","previous":{"title":"Common Patterns","permalink":"/docs/core-concepts/common-patterns"},"next":{"title":"Best Practices","permalink":"/docs/core-concepts/best-practices"}}');var t=i(4848),s=i(8453);const a={title:"Streaming vs. Buffering",description:"Understand the differences between streaming and buffering in NPipeline and how they impact performance and memory usage.",sidebar_position:8},l="Streaming vs. Buffering",c={},o=[{value:"Visual Comparison: Streaming vs Buffering",id:"visual-comparison-streaming-vs-buffering",level:2},{value:"Trade-offs Comparison",id:"trade-offs-comparison",level:3},{value:"Streaming",id:"streaming",level:2},{value:"Characteristics of Streaming",id:"characteristics-of-streaming",level:3},{value:"Example: A Pure Streaming Pipeline",id:"example-a-pure-streaming-pipeline",level:3},{value:"Buffering",id:"buffering",level:2},{value:"Characteristics of Buffering",id:"characteristics-of-buffering",level:3},{value:"Example: Introducing Buffering with a Batching Node",id:"example-introducing-buffering-with-a-batching-node",level:3},{value:"Choosing Between Streaming and Buffering",id:"choosing-between-streaming-and-buffering",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"streaming-vs-buffering",children:"Streaming vs. Buffering"})}),"\n",(0,t.jsx)(n.p,{children:"In NPipeline, understanding the concepts of streaming and buffering is crucial for designing efficient and performant data pipelines. These concepts dictate how data items are handled as they move between nodes, directly impacting memory consumption, latency, and overall throughput."}),"\n",(0,t.jsx)(n.h2,{id:"visual-comparison-streaming-vs-buffering",children:"Visual Comparison: Streaming vs Buffering"}),"\n",(0,t.jsx)(n.mermaid,{value:'graph TB\r\n    subgraph "Streaming Approach"\r\n        S1[Source] --\x3e |Item 1| T1[Transform]\r\n        S1 --\x3e |Item 2| T1\r\n        S1 --\x3e |Item 3| T1\r\n        T1 --\x3e |Processed Item 1| SINK1[Sink]\r\n        T1 --\x3e |Processed Item 2| SINK1\r\n        T1 --\x3e |Processed Item 3| SINK1\r\n\r\n        style S1 fill:#e1f5fe\r\n        style T1 fill:#e8f5e9\r\n        style SINK1 fill:#fff3e0\r\n    end\r\n\r\n    subgraph "Buffering Approach"\r\n        S2[Source] --\x3e |Items 1,2,3| BUFFER[Buffer/Batch Node]\r\n        BUFFER --\x3e |Batch &#91;1,2,3&#93;| T2[Transform]\r\n        T2 --\x3e |Processed Batch| SINK2[Sink]\r\n\r\n        style S2 fill:#e1f5fe\r\n        style BUFFER fill:#fce4ec\r\n        style T2 fill:#e8f5e9\r\n        style SINK2 fill:#fff3e0\r\n    end'}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Figure 1: Side-by-side comparison of streaming and buffering data flow patterns. In streaming, items are processed individually as they arrive, while in buffering, items are collected into batches before processing."})}),"\n",(0,t.jsx)(n.h3,{id:"trade-offs-comparison",children:"Trade-offs Comparison"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Aspect"}),(0,t.jsx)(n.th,{children:"Streaming"}),(0,t.jsx)(n.th,{children:"Buffering"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Memory Usage"})}),(0,t.jsx)(n.td,{children:"Low - only a few items in memory at once"}),(0,t.jsx)(n.td,{children:"Higher - accumulates items before processing"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Latency"})}),(0,t.jsx)(n.td,{children:"Low - immediate processing of each item"}),(0,t.jsx)(n.td,{children:"Higher - waits for batch completion"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Processing Model"})}),(0,t.jsx)(n.td,{children:"Continuous, per-item processing"}),(0,t.jsx)(n.td,{children:"Batch-oriented processing"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Overhead"})}),(0,t.jsx)(n.td,{children:"Higher per-item processing cost"}),(0,t.jsx)(n.td,{children:"Lower per-item cost due to batching"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Use Cases"})}),(0,t.jsx)(n.td,{children:"Real-time analytics, continuous processing"}),(0,t.jsx)(n.td,{children:"Batch processing, database writes, file I/O"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Error Handling"})}),(0,t.jsx)(n.td,{children:"Errors isolated to individual items"}),(0,t.jsx)(n.td,{children:"Errors may affect entire batch"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:(0,t.jsx)(n.em,{children:"Table 1: Trade-offs between streaming and buffering approaches. Each approach has distinct advantages depending on your specific requirements."})}),"\n",(0,t.jsx)(n.h2,{id:"streaming",children:"Streaming"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Streaming"})," refers to the processing of data items one by one, or in small, continuous batches, as they arrive. In a streaming pipeline, data is not accumulated in memory before being passed to the next stage. Instead, each item (or a small group of items) is processed and immediately forwarded."]}),"\n",(0,t.jsx)(n.h3,{id:"characteristics-of-streaming",children:"Characteristics of Streaming"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Low Memory Footprint"}),": Only a few data items are held in memory at any given time."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Low Latency"}),": Data is processed and moved through the pipeline quickly, reducing delays."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Continuous Processing"}),": Ideal for real-time data processing and long-running pipelines."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Push" Model'}),": Data is pushed from upstream nodes to downstream nodes as soon as it's ready."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["Most NPipeline nodes inherently support streaming behavior, especially ",(0,t.jsx)(n.code,{children:"ISourceNode"})," (which ",(0,t.jsx)(n.code,{children:"yield"}),"s items) and ",(0,t.jsx)(n.code,{children:"ITransformNode"})," (which also ",(0,t.jsx)(n.code,{children:"yield"}),"s transformed items)."]}),"\n",(0,t.jsx)(n.h3,{id:"example-a-pure-streaming-pipeline",children:"Example: A Pure Streaming Pipeline"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using NPipeline;\r\n\r\npublic sealed record EventData(int Id, string Message);\r\n\r\npublic sealed class StreamSource : SourceNode<EventData>\r\n{\r\n    public async IAsyncEnumerable<EventData> ExecuteAsync(CancellationToken cancellationToken = default)\r\n    {\r\n        for (int i = 0; i < 5; i++)\r\n        {\r\n            if (cancellationToken.IsCancellationRequested) yield break;\r\n            var data = new EventData(i, $"Event {i}");\r\n            Console.WriteLine($"Source: Producing {data.Id}");\r\n            yield return data;\r\n            await Task.Delay(10, cancellationToken); // Simulate fast stream\r\n        }\r\n    }\r\n}\r\n\r\npublic sealed class StreamTransform : TransformNode<EventData, string>\r\n{\r\n    public async IAsyncEnumerable<string> ExecuteAsync(IAsyncEnumerable<EventData> input, CancellationToken cancellationToken = default)\r\n    {\r\n        await foreach (var item in input.WithCancellation(cancellationToken))\r\n        {\r\n            if (cancellationToken.IsCancellationRequested) yield break;\r\n            var transformed = $"Processed: {item.Message.ToUpper()}";\r\n            Console.WriteLine($"Transform: {item.Id} -> {transformed}");\r\n            yield return transformed;\r\n        }\r\n    }\r\n}\r\n\r\npublic sealed class StreamSink : SinkNode<string>\r\n{\r\n    public async Task ExecuteAsync(IAsyncEnumerable<string> input, CancellationToken cancellationToken = default)\r\n    {\r\n        await foreach (var item in input.WithCancellation(cancellationToken))\r\n        {\r\n            if (cancellationToken.IsCancellationRequested) break;\r\n            Console.WriteLine($"Sink: Consumed \'{item}\'");\r\n        }\r\n    }\r\n}\r\n\r\npublic static class Program\r\n{\r\n    public static async Task Main(string[] args)\r\n    {\r\n        var context = new PipelineContext();\r\n        var runner = PipelineRunner.Create();\r\n\r\n        Console.WriteLine("Starting streaming pipeline...");\r\n        await runner.RunAsync<StreamingPipelineDefinition>(context);\r\n        Console.WriteLine("Streaming pipeline finished.");\r\n    }\r\n}\r\n\r\npublic sealed class StreamingPipelineDefinition : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        var sourceHandle = builder.AddSource<StreamSource, EventData>("source");\r\n        var transformHandle = builder.AddTransform<StreamTransform, EventData, string>("transform");\r\n        var sinkHandle = builder.AddSink<StreamSink, string>("sink");\r\n\r\n        builder.Connect(sourceHandle, transformHandle);\r\n        builder.Connect(transformHandle, sinkHandle);\r\n    }\r\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["In this example, each ",(0,t.jsx)(n.code,{children:"EventData"})," item is produced by the ",(0,t.jsx)(n.code,{children:"StreamSource"}),", immediately transformed by ",(0,t.jsx)(n.code,{children:"StreamTransform"}),", and then immediately consumed by ",(0,t.jsx)(n.code,{children:"StreamSink"}),". Data flows continuously without significant accumulation."]}),"\n",(0,t.jsx)(n.h2,{id:"buffering",children:"Buffering"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Buffering"})," involves accumulating a certain amount of data in memory before processing or forwarding it to the next stage. This can be done explicitly by specific nodes (e.g., batching nodes) or implicitly by operations that require all data to be present before proceeding (e.g., certain aggregation operations)."]}),"\n",(0,t.jsx)(n.h3,{id:"characteristics-of-buffering",children:"Characteristics of Buffering"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Higher Memory Footprint"}),": Data is temporarily stored in memory, potentially increasing memory usage."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Increased Latency"}),": Processing is delayed until a buffer is full or a specific condition is met."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Batch Processing"}),": Suitable for operations that benefit from processing data in chunks (e.g., database inserts, file writes, certain analytical functions)."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:'"Pull" or "Batch-and-Push" Model'}),": Data might be pulled into a buffer and then pushed as a batch."]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"While buffering can introduce latency and higher memory usage, it can also lead to increased throughput for certain operations by reducing overhead (e.g., fewer database transactions for batch inserts)."}),"\n",(0,t.jsx)(n.h3,{id:"example-introducing-buffering-with-a-batching-node",children:"Example: Introducing Buffering with a Batching Node"}),"\n",(0,t.jsxs)(n.p,{children:["NPipeline provides specific nodes, like ",(0,t.jsx)(n.a,{href:"src/NPipeline/Nodes/Batching/BatchingNode.cs",children:(0,t.jsx)(n.code,{children:"BatchingNode<T>"})}),", to introduce explicit buffering."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-csharp",children:'using NPipeline;\r\nusing NPipeline.Interfaces;\r\nusing NPipeline.Nodes;\r\n\r\npublic sealed record Message(int Id, string Content);\r\n\r\npublic sealed class CountingSource : SourceNode<Message>\r\n{\r\n    public async IAsyncEnumerable<Message> ExecuteAsync(CancellationToken cancellationToken = default)\r\n    {\r\n        for (int i = 0; i < 10; i++)\r\n        {\r\n            if (cancellationToken.IsCancellationRequested) yield break;\r\n            var msg = new Message(i, $"Item {i}");\r\n            Console.WriteLine($"Source: Producing {msg.Id}");\r\n            yield return msg;\r\n            await Task.Delay(10, cancellationToken);\r\n        }\r\n    }\r\n}\r\n\r\npublic sealed class BatchSink : SinkNode<IReadOnlyList<Message>>\r\n{\r\n    public async Task ExecuteAsync(IAsyncEnumerable<IReadOnlyList<Message>> input, CancellationToken cancellationToken = default)\r\n    {\r\n        await foreach (var batch in input.WithCancellation(cancellationToken))\r\n        {\r\n            if (cancellationToken.IsCancellationRequested) break;\r\n            Console.WriteLine($"Sink: Consumed a batch of {batch.Count} items.");\r\n            foreach (var item in batch)\r\n            {\r\n                Console.WriteLine($"  - Processed item {item.Id}: {item.Content}");\r\n            }\r\n            await Task.Delay(50, cancellationToken); // Simulate batch processing time\r\n        }\r\n    }\r\n}\r\n\r\npublic static class Program\r\n{\r\n    public static async Task Main(string[] args)\r\n    {\r\n        var context = new PipelineContext();\r\n        var runner = PipelineRunner.Create();\r\n\r\n        Console.WriteLine("Starting buffering pipeline...");\r\n        await runner.RunAsync<BufferingPipelineDefinition>(context);\r\n        Console.WriteLine("Buffering pipeline finished.");\r\n    }\r\n}\r\n\r\npublic sealed class BufferingPipelineDefinition : IPipelineDefinition\r\n{\r\n    public void Define(PipelineBuilder builder, PipelineContext context)\r\n    {\r\n        var sourceHandle = builder.AddSource<CountingSource, Message>("source");\r\n        var batchHandle = builder.AddTransform<BatchingNode<Message>, Message, IReadOnlyList<Message>>("batcher");\r\n        var sinkHandle = builder.AddSink<BatchSink, IReadOnlyList<Message>>("sink");\r\n\r\n        builder.Connect(sourceHandle, batchHandle);\r\n        builder.Connect(batchHandle, sinkHandle);\r\n    }\r\n}\n'})}),"\n",(0,t.jsxs)(n.p,{children:["In this example, the ",(0,t.jsx)(n.code,{children:"BatchingNode"})," collects ",(0,t.jsx)(n.code,{children:"Message"})," items until it has 3 items or 1 second passes, then it forwards the entire list (",(0,t.jsx)(n.code,{children:"IReadOnlyList<Message>"}),") to the ",(0,t.jsx)(n.code,{children:"BatchSink"}),". This demonstrates explicit buffering."]}),"\n",(0,t.jsx)(n.h2,{id:"choosing-between-streaming-and-buffering",children:"Choosing Between Streaming and Buffering"}),"\n",(0,t.jsx)(n.p,{children:"The choice between streaming and buffering depends on your specific use case and requirements:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{style:{textAlign:"left"},children:"Feature"}),(0,t.jsx)(n.th,{style:{textAlign:"left"},children:"Streaming"}),(0,t.jsx)(n.th,{style:{textAlign:"left"},children:"Buffering"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.strong,{children:"Memory Usage"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Low"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Potentially High"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.strong,{children:"Latency"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Low"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Higher"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.strong,{children:"Throughput"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"High for continuous data, per-item overhead"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Can be higher for batch-optimized operations"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.strong,{children:"Use Cases"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Real-time analytics, continuous processing"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Batch processing, database writes, file I/O"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{style:{textAlign:"left"},children:(0,t.jsx)(n.strong,{children:"Fault Tolerance"})}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Errors can be handled per item"}),(0,t.jsx)(n.td,{style:{textAlign:"left"},children:"Errors might affect an entire batch"})]})]})]}),"\n",(0,t.jsx)(n.p,{children:"NPipeline is designed to support both paradigms, allowing you to combine streaming and buffering nodes to optimize your data flows for various scenarios."}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/core-concepts/resilience/error-handling",children:"Error Handling"})}),": Learn how to manage errors effectively in both streaming and buffering pipelines."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:(0,t.jsx)(n.a,{href:"/docs/core-concepts/nodes/aggregation",children:"Node Types (e.g., Aggregation)"})}),": Explore nodes that inherently involve buffering for their operations."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>l});var r=i(6540);const t={},s=r.createContext(t);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);